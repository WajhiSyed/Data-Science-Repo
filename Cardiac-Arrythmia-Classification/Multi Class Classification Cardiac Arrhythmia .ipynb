{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'>Cardiac Arrhythmia Multi-Class Classification \n",
    "\n",
    "# Process Flow :\n",
    "\n",
    "Analyze data and address missing data if there is any. \n",
    "\n",
    "Decide aboute a good evaluation strategy and justify your choice. \n",
    "\n",
    "Find the best parameters for the following classification models: \n",
    "- KNN classifcation \n",
    "- Logistic Regression\n",
    "- Linear Supprt Vector Machine\n",
    "- Kerenilzed Support Vector Machine\n",
    "- Decision Tree\n",
    "- Random Forest \n",
    "\n",
    "Then use different bagging and boosting methods to boost the results. Do you see any significant change? \n",
    "\n",
    "use data reduction method to reduce the size of data, and agian try above models. Do you get better results? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "kFold = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('cardiac_arrhythmia.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "      <th>279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>91</td>\n",
       "      <td>193</td>\n",
       "      <td>371</td>\n",
       "      <td>174</td>\n",
       "      <td>121</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>165</td>\n",
       "      <td>64</td>\n",
       "      <td>81</td>\n",
       "      <td>174</td>\n",
       "      <td>401</td>\n",
       "      <td>149</td>\n",
       "      <td>39</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>95</td>\n",
       "      <td>138</td>\n",
       "      <td>163</td>\n",
       "      <td>386</td>\n",
       "      <td>185</td>\n",
       "      <td>102</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>94</td>\n",
       "      <td>100</td>\n",
       "      <td>202</td>\n",
       "      <td>380</td>\n",
       "      <td>179</td>\n",
       "      <td>143</td>\n",
       "      <td>28</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>80</td>\n",
       "      <td>88</td>\n",
       "      <td>181</td>\n",
       "      <td>360</td>\n",
       "      <td>177</td>\n",
       "      <td>103</td>\n",
       "      <td>-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 280 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9   ...   270   271  272  273  \\\n",
       "0   75    0  190   80   91  193  371  174  121  -16 ...   0.0   9.0 -0.9  0.0   \n",
       "1   56    1  165   64   81  174  401  149   39   25 ...   0.0   8.5  0.0  0.0   \n",
       "2   54    0  172   95  138  163  386  185  102   96 ...   0.0   9.5 -2.4  0.0   \n",
       "3   55    0  175   94  100  202  380  179  143   28 ...   0.0  12.2 -2.2  0.0   \n",
       "4   75    0  190   80   88  181  360  177  103  -16 ...   0.0  13.1 -3.6  0.0   \n",
       "\n",
       "  274  275  276   277   278  279  \n",
       "0   0  0.9  2.9  23.3  49.4    8  \n",
       "1   0  0.2  2.1  20.4  38.8    6  \n",
       "2   0  0.3  3.4  12.3  49.0   10  \n",
       "3   0  0.4  2.6  34.6  61.6    1  \n",
       "4   0 -0.1  3.9  25.4  62.8    7  \n",
       "\n",
       "[5 rows x 280 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(452, 280)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       8\n",
       "1       6\n",
       "2      10\n",
       "3       1\n",
       "4       7\n",
       "5      14\n",
       "6       1\n",
       "7       1\n",
       "8       1\n",
       "9      10\n",
       "10      3\n",
       "11      1\n",
       "12     10\n",
       "13      6\n",
       "14      1\n",
       "15      1\n",
       "16     10\n",
       "17      1\n",
       "18      1\n",
       "19      1\n",
       "20      1\n",
       "21      1\n",
       "22      1\n",
       "23      1\n",
       "24      1\n",
       "25     16\n",
       "26     14\n",
       "27     10\n",
       "28      2\n",
       "29      2\n",
       "       ..\n",
       "422     1\n",
       "423     1\n",
       "424     9\n",
       "425     1\n",
       "426     1\n",
       "427    10\n",
       "428     1\n",
       "429    16\n",
       "430    10\n",
       "431     6\n",
       "432    10\n",
       "433     3\n",
       "434     1\n",
       "435     1\n",
       "436     1\n",
       "437     1\n",
       "438     1\n",
       "439     1\n",
       "440     1\n",
       "441     1\n",
       "442     1\n",
       "443    10\n",
       "444     1\n",
       "445     1\n",
       "446     1\n",
       "447     1\n",
       "448    10\n",
       "449     2\n",
       "450     1\n",
       "451     1\n",
       "Name: 279, Length: 452, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[279]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for i in range(0,452):\n",
    "    for j in range(0,280):\n",
    "        if (data.iloc[i,j]=='?'):\n",
    "            count =count+1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing '?' with Null "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,452):\n",
    "    for j in range(0,280):\n",
    "        if (data.iloc[i,j]=='?'):\n",
    "            data.iloc[i,j] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "5        0\n",
       "6        0\n",
       "7        0\n",
       "8        0\n",
       "9        0\n",
       "10       8\n",
       "11      22\n",
       "12       1\n",
       "13     376\n",
       "14       1\n",
       "15       0\n",
       "16       0\n",
       "17       0\n",
       "18       0\n",
       "19       0\n",
       "20       0\n",
       "21       0\n",
       "22       0\n",
       "23       0\n",
       "24       0\n",
       "25       0\n",
       "26       0\n",
       "27       0\n",
       "28       0\n",
       "29       0\n",
       "      ... \n",
       "250      0\n",
       "251      0\n",
       "252      0\n",
       "253      0\n",
       "254      0\n",
       "255      0\n",
       "256      0\n",
       "257      0\n",
       "258      0\n",
       "259      0\n",
       "260      0\n",
       "261      0\n",
       "262      0\n",
       "263      0\n",
       "264      0\n",
       "265      0\n",
       "266      0\n",
       "267      0\n",
       "268      0\n",
       "269      0\n",
       "270      0\n",
       "271      0\n",
       "272      0\n",
       "273      0\n",
       "274      0\n",
       "275      0\n",
       "276      0\n",
       "277      0\n",
       "278      0\n",
       "279      0\n",
       "Length: 280, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Droping column 13 with lots of missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns = 13, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing other missing data using KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing row 1/452 with 0 missing, elapsed time: 0.432\n",
      "Imputing row 101/452 with 0 missing, elapsed time: 0.432\n",
      "Imputing row 201/452 with 1 missing, elapsed time: 0.432\n",
      "Imputing row 301/452 with 1 missing, elapsed time: 0.432\n",
      "Imputing row 401/452 with 0 missing, elapsed time: 0.436\n"
     ]
    }
   ],
   "source": [
    "import fancyimpute\n",
    "data_no_missing = fancyimpute.KNN(k=5).complete(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.isnull(data_no_missing).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_missing = pd.DataFrame(data_no_missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1      2     3      4      5      6      7      8     9    ...   269  \\\n",
       "0  75.0  0.0  190.0  80.0   91.0  193.0  371.0  174.0  121.0 -16.0  ...   0.0   \n",
       "1  56.0  1.0  165.0  64.0   81.0  174.0  401.0  149.0   39.0  25.0  ...   0.0   \n",
       "2  54.0  0.0  172.0  95.0  138.0  163.0  386.0  185.0  102.0  96.0  ...   0.0   \n",
       "3  55.0  0.0  175.0  94.0  100.0  202.0  380.0  179.0  143.0  28.0  ...   0.0   \n",
       "4  75.0  0.0  190.0  80.0   88.0  181.0  360.0  177.0  103.0 -16.0  ...   0.0   \n",
       "\n",
       "    270  271  272  273  274  275   276   277   278  \n",
       "0   9.0 -0.9  0.0  0.0  0.9  2.9  23.3  49.4   8.0  \n",
       "1   8.5  0.0  0.0  0.0  0.2  2.1  20.4  38.8   6.0  \n",
       "2   9.5 -2.4  0.0  0.0  0.3  3.4  12.3  49.0  10.0  \n",
       "3  12.2 -2.2  0.0  0.0  0.4  2.6  34.6  61.6   1.0  \n",
       "4  13.1 -3.6  0.0  0.0 -0.1  3.9  25.4  62.8   7.0  \n",
       "\n",
       "[5 rows x 279 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_missing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     245\n",
       "10.0     50\n",
       "2.0      44\n",
       "6.0      25\n",
       "16.0     22\n",
       "4.0      15\n",
       "3.0      15\n",
       "5.0      13\n",
       "9.0       9\n",
       "15.0      5\n",
       "14.0      4\n",
       "7.0       3\n",
       "8.0       2\n",
       "Name: 278, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_missing[278].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "      <th>278</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.059042</td>\n",
       "      <td>-0.109458</td>\n",
       "      <td>0.381555</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.041149</td>\n",
       "      <td>0.195691</td>\n",
       "      <td>0.025654</td>\n",
       "      <td>0.099755</td>\n",
       "      <td>-0.265868</td>\n",
       "      <td>...</td>\n",
       "      <td>0.164816</td>\n",
       "      <td>-0.158009</td>\n",
       "      <td>0.082376</td>\n",
       "      <td>0.090413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.037877</td>\n",
       "      <td>-0.271504</td>\n",
       "      <td>0.018043</td>\n",
       "      <td>-0.199728</td>\n",
       "      <td>-0.092381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.059042</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124685</td>\n",
       "      <td>-0.248104</td>\n",
       "      <td>-0.337101</td>\n",
       "      <td>-0.046771</td>\n",
       "      <td>0.072052</td>\n",
       "      <td>-0.184736</td>\n",
       "      <td>-0.081051</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>...</td>\n",
       "      <td>0.230938</td>\n",
       "      <td>-0.042638</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>0.031688</td>\n",
       "      <td>0.046605</td>\n",
       "      <td>-0.178080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.109458</td>\n",
       "      <td>-0.124685</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.074957</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>-0.237314</td>\n",
       "      <td>-0.038411</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018876</td>\n",
       "      <td>-0.073439</td>\n",
       "      <td>-0.091361</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.067670</td>\n",
       "      <td>-0.008471</td>\n",
       "      <td>-0.090370</td>\n",
       "      <td>-0.092235</td>\n",
       "      <td>0.006648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.381555</td>\n",
       "      <td>-0.248104</td>\n",
       "      <td>-0.074957</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>0.119826</td>\n",
       "      <td>0.118657</td>\n",
       "      <td>0.149987</td>\n",
       "      <td>0.120668</td>\n",
       "      <td>-0.173355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.050471</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>0.047448</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.046278</td>\n",
       "      <td>-0.144721</td>\n",
       "      <td>0.062285</td>\n",
       "      <td>-0.050682</td>\n",
       "      <td>-0.090151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.004032</td>\n",
       "      <td>-0.337101</td>\n",
       "      <td>-0.006329</td>\n",
       "      <td>0.100094</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>0.218681</td>\n",
       "      <td>0.397435</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>-0.146043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.198941</td>\n",
       "      <td>0.091384</td>\n",
       "      <td>-0.228688</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.065596</td>\n",
       "      <td>-0.222170</td>\n",
       "      <td>0.129796</td>\n",
       "      <td>-0.082791</td>\n",
       "      <td>0.323879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.041149</td>\n",
       "      <td>-0.046771</td>\n",
       "      <td>0.013601</td>\n",
       "      <td>0.119826</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.079371</td>\n",
       "      <td>0.074618</td>\n",
       "      <td>0.670865</td>\n",
       "      <td>-0.012412</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>-0.074180</td>\n",
       "      <td>0.068594</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.143272</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>-0.027691</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>-0.099954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.195691</td>\n",
       "      <td>0.072052</td>\n",
       "      <td>-0.237314</td>\n",
       "      <td>0.118657</td>\n",
       "      <td>0.218681</td>\n",
       "      <td>0.079371</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.166711</td>\n",
       "      <td>0.063044</td>\n",
       "      <td>-0.031786</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035215</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>-0.002351</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.034971</td>\n",
       "      <td>-0.038884</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.150979</td>\n",
       "      <td>0.028305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.025654</td>\n",
       "      <td>-0.184736</td>\n",
       "      <td>-0.038411</td>\n",
       "      <td>0.149987</td>\n",
       "      <td>0.397435</td>\n",
       "      <td>0.074618</td>\n",
       "      <td>0.166711</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>-0.099283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068035</td>\n",
       "      <td>0.070535</td>\n",
       "      <td>-0.046868</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.049175</td>\n",
       "      <td>-0.184846</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>-0.014430</td>\n",
       "      <td>0.097625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.099755</td>\n",
       "      <td>-0.081051</td>\n",
       "      <td>0.029025</td>\n",
       "      <td>0.120668</td>\n",
       "      <td>0.049682</td>\n",
       "      <td>0.670865</td>\n",
       "      <td>0.063044</td>\n",
       "      <td>0.060302</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.062760</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010142</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>-0.091258</td>\n",
       "      <td>0.096772</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>-0.016365</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>-0.122003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.265868</td>\n",
       "      <td>0.069434</td>\n",
       "      <td>0.061539</td>\n",
       "      <td>-0.173355</td>\n",
       "      <td>-0.146043</td>\n",
       "      <td>-0.012412</td>\n",
       "      <td>-0.031786</td>\n",
       "      <td>-0.099283</td>\n",
       "      <td>-0.062760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.179725</td>\n",
       "      <td>0.292155</td>\n",
       "      <td>0.295283</td>\n",
       "      <td>-0.071495</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.082009</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>0.297780</td>\n",
       "      <td>0.255203</td>\n",
       "      <td>0.019585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.008095</td>\n",
       "      <td>-0.141667</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>-0.046153</td>\n",
       "      <td>0.031114</td>\n",
       "      <td>0.114645</td>\n",
       "      <td>0.159971</td>\n",
       "      <td>0.040838</td>\n",
       "      <td>0.143195</td>\n",
       "      <td>-0.051925</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034564</td>\n",
       "      <td>-0.052905</td>\n",
       "      <td>-0.055454</td>\n",
       "      <td>0.004383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.071090</td>\n",
       "      <td>0.157539</td>\n",
       "      <td>-0.062546</td>\n",
       "      <td>0.084336</td>\n",
       "      <td>0.005178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.046294</td>\n",
       "      <td>0.013971</td>\n",
       "      <td>-0.106477</td>\n",
       "      <td>-0.026709</td>\n",
       "      <td>0.040207</td>\n",
       "      <td>0.008069</td>\n",
       "      <td>-0.091543</td>\n",
       "      <td>-0.111296</td>\n",
       "      <td>0.054635</td>\n",
       "      <td>0.054649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007202</td>\n",
       "      <td>-0.065499</td>\n",
       "      <td>0.063194</td>\n",
       "      <td>-0.003007</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.212522</td>\n",
       "      <td>0.039697</td>\n",
       "      <td>-0.036134</td>\n",
       "      <td>-0.013432</td>\n",
       "      <td>-0.012897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.285805</td>\n",
       "      <td>0.030865</td>\n",
       "      <td>0.024969</td>\n",
       "      <td>-0.212601</td>\n",
       "      <td>-0.075626</td>\n",
       "      <td>0.020685</td>\n",
       "      <td>-0.084250</td>\n",
       "      <td>-0.107960</td>\n",
       "      <td>-0.073149</td>\n",
       "      <td>0.709528</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.222315</td>\n",
       "      <td>0.169013</td>\n",
       "      <td>0.132601</td>\n",
       "      <td>-0.066043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.052213</td>\n",
       "      <td>0.194113</td>\n",
       "      <td>0.101686</td>\n",
       "      <td>0.214848</td>\n",
       "      <td>0.069403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.192239</td>\n",
       "      <td>0.062729</td>\n",
       "      <td>0.286553</td>\n",
       "      <td>-0.174988</td>\n",
       "      <td>-0.006001</td>\n",
       "      <td>-0.045091</td>\n",
       "      <td>-0.654681</td>\n",
       "      <td>0.016251</td>\n",
       "      <td>0.043433</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051391</td>\n",
       "      <td>-0.118857</td>\n",
       "      <td>-0.236425</td>\n",
       "      <td>0.032815</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.179483</td>\n",
       "      <td>-0.083605</td>\n",
       "      <td>-0.249300</td>\n",
       "      <td>-0.234168</td>\n",
       "      <td>0.008331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.016675</td>\n",
       "      <td>-0.129383</td>\n",
       "      <td>-0.001940</td>\n",
       "      <td>0.038332</td>\n",
       "      <td>0.118847</td>\n",
       "      <td>0.017437</td>\n",
       "      <td>0.018153</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>0.072590</td>\n",
       "      <td>-0.055314</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.342547</td>\n",
       "      <td>-0.118037</td>\n",
       "      <td>-0.028179</td>\n",
       "      <td>-0.042085</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.086201</td>\n",
       "      <td>-0.037790</td>\n",
       "      <td>-0.170610</td>\n",
       "      <td>-0.163450</td>\n",
       "      <td>0.003965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.199533</td>\n",
       "      <td>0.016725</td>\n",
       "      <td>-0.091314</td>\n",
       "      <td>0.117731</td>\n",
       "      <td>0.310412</td>\n",
       "      <td>-0.067438</td>\n",
       "      <td>0.296504</td>\n",
       "      <td>0.251603</td>\n",
       "      <td>-0.025343</td>\n",
       "      <td>-0.271659</td>\n",
       "      <td>...</td>\n",
       "      <td>0.195220</td>\n",
       "      <td>0.126898</td>\n",
       "      <td>0.199933</td>\n",
       "      <td>0.094711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.103729</td>\n",
       "      <td>-0.241676</td>\n",
       "      <td>0.484570</td>\n",
       "      <td>0.159676</td>\n",
       "      <td>0.042674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.120405</td>\n",
       "      <td>-0.149390</td>\n",
       "      <td>0.040893</td>\n",
       "      <td>-0.023805</td>\n",
       "      <td>0.185647</td>\n",
       "      <td>0.053189</td>\n",
       "      <td>-0.084368</td>\n",
       "      <td>-0.037765</td>\n",
       "      <td>0.035735</td>\n",
       "      <td>0.213609</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.112477</td>\n",
       "      <td>0.025226</td>\n",
       "      <td>-0.302136</td>\n",
       "      <td>-0.081102</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008272</td>\n",
       "      <td>0.099483</td>\n",
       "      <td>-0.267412</td>\n",
       "      <td>-0.102082</td>\n",
       "      <td>0.195198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.028642</td>\n",
       "      <td>-0.020588</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.119989</td>\n",
       "      <td>0.051225</td>\n",
       "      <td>0.012168</td>\n",
       "      <td>0.014332</td>\n",
       "      <td>0.013292</td>\n",
       "      <td>-0.011175</td>\n",
       "      <td>0.055497</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.037438</td>\n",
       "      <td>0.034874</td>\n",
       "      <td>0.040968</td>\n",
       "      <td>-0.007184</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.008735</td>\n",
       "      <td>0.040192</td>\n",
       "      <td>0.060671</td>\n",
       "      <td>0.073527</td>\n",
       "      <td>0.063999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.152401</td>\n",
       "      <td>-0.088119</td>\n",
       "      <td>-0.082112</td>\n",
       "      <td>0.145418</td>\n",
       "      <td>0.415102</td>\n",
       "      <td>-0.034179</td>\n",
       "      <td>0.233446</td>\n",
       "      <td>0.287783</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>-0.258226</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053776</td>\n",
       "      <td>0.126330</td>\n",
       "      <td>0.039008</td>\n",
       "      <td>-0.007327</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003834</td>\n",
       "      <td>-0.198374</td>\n",
       "      <td>0.309029</td>\n",
       "      <td>0.072924</td>\n",
       "      <td>0.042764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.044455</td>\n",
       "      <td>0.042517</td>\n",
       "      <td>0.004834</td>\n",
       "      <td>0.118853</td>\n",
       "      <td>0.024790</td>\n",
       "      <td>-0.163104</td>\n",
       "      <td>-0.103370</td>\n",
       "      <td>0.018588</td>\n",
       "      <td>-0.164281</td>\n",
       "      <td>0.005523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023960</td>\n",
       "      <td>0.084864</td>\n",
       "      <td>0.034307</td>\n",
       "      <td>-0.003746</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.137654</td>\n",
       "      <td>-0.033795</td>\n",
       "      <td>0.060650</td>\n",
       "      <td>0.024028</td>\n",
       "      <td>-0.020115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.130713</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>-0.014208</td>\n",
       "      <td>-0.002364</td>\n",
       "      <td>0.196260</td>\n",
       "      <td>0.054418</td>\n",
       "      <td>0.076723</td>\n",
       "      <td>0.072653</td>\n",
       "      <td>0.075415</td>\n",
       "      <td>0.006812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.053816</td>\n",
       "      <td>-0.024634</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>0.329603</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.065449</td>\n",
       "      <td>-0.136788</td>\n",
       "      <td>0.147827</td>\n",
       "      <td>-0.019770</td>\n",
       "      <td>0.007675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.008544</td>\n",
       "      <td>-0.074608</td>\n",
       "      <td>0.014277</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>0.010501</td>\n",
       "      <td>0.080529</td>\n",
       "      <td>-0.022431</td>\n",
       "      <td>-0.071352</td>\n",
       "      <td>-0.070562</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038777</td>\n",
       "      <td>-0.079512</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>-0.008413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>-0.066996</td>\n",
       "      <td>-0.079552</td>\n",
       "      <td>-0.118700</td>\n",
       "      <td>-0.035569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.089284</td>\n",
       "      <td>-0.006820</td>\n",
       "      <td>-0.010213</td>\n",
       "      <td>-0.010742</td>\n",
       "      <td>0.022065</td>\n",
       "      <td>-0.109620</td>\n",
       "      <td>0.012578</td>\n",
       "      <td>-0.032682</td>\n",
       "      <td>0.052964</td>\n",
       "      <td>-0.060491</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>-0.057615</td>\n",
       "      <td>0.025241</td>\n",
       "      <td>-0.005303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.079663</td>\n",
       "      <td>-0.008066</td>\n",
       "      <td>-0.030277</td>\n",
       "      <td>-0.028413</td>\n",
       "      <td>0.032097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.056860</td>\n",
       "      <td>-0.073835</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.009372</td>\n",
       "      <td>0.165412</td>\n",
       "      <td>0.005726</td>\n",
       "      <td>0.091543</td>\n",
       "      <td>0.132141</td>\n",
       "      <td>-0.010348</td>\n",
       "      <td>-0.064163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033923</td>\n",
       "      <td>0.016373</td>\n",
       "      <td>-0.019755</td>\n",
       "      <td>-0.005303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.050857</td>\n",
       "      <td>-0.087627</td>\n",
       "      <td>0.117004</td>\n",
       "      <td>0.015253</td>\n",
       "      <td>0.032097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.031761</td>\n",
       "      <td>-0.104651</td>\n",
       "      <td>0.006520</td>\n",
       "      <td>0.070300</td>\n",
       "      <td>0.243685</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.083706</td>\n",
       "      <td>0.200566</td>\n",
       "      <td>0.014635</td>\n",
       "      <td>0.045439</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016548</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.038137</td>\n",
       "      <td>-0.007516</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.038059</td>\n",
       "      <td>-0.119225</td>\n",
       "      <td>0.159007</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.083056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.039709</td>\n",
       "      <td>-0.139389</td>\n",
       "      <td>-0.013874</td>\n",
       "      <td>0.086654</td>\n",
       "      <td>0.093043</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>-0.061630</td>\n",
       "      <td>0.005486</td>\n",
       "      <td>-0.000912</td>\n",
       "      <td>0.126140</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.498626</td>\n",
       "      <td>0.073974</td>\n",
       "      <td>0.100968</td>\n",
       "      <td>0.007432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098454</td>\n",
       "      <td>-0.102425</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>-0.063413</td>\n",
       "      <td>-0.003612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.206806</td>\n",
       "      <td>0.016074</td>\n",
       "      <td>-0.064018</td>\n",
       "      <td>0.131662</td>\n",
       "      <td>0.261717</td>\n",
       "      <td>-0.003799</td>\n",
       "      <td>0.253075</td>\n",
       "      <td>0.151196</td>\n",
       "      <td>-0.044184</td>\n",
       "      <td>0.306674</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.225314</td>\n",
       "      <td>0.297373</td>\n",
       "      <td>-0.011813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>-0.140302</td>\n",
       "      <td>0.528282</td>\n",
       "      <td>0.255147</td>\n",
       "      <td>0.048367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.130212</td>\n",
       "      <td>-0.117475</td>\n",
       "      <td>-0.010917</td>\n",
       "      <td>-0.055522</td>\n",
       "      <td>0.292268</td>\n",
       "      <td>-0.027114</td>\n",
       "      <td>0.055224</td>\n",
       "      <td>0.160626</td>\n",
       "      <td>0.089904</td>\n",
       "      <td>-0.433352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059455</td>\n",
       "      <td>-0.140348</td>\n",
       "      <td>-0.404699</td>\n",
       "      <td>-0.016526</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.053551</td>\n",
       "      <td>0.017808</td>\n",
       "      <td>-0.319503</td>\n",
       "      <td>-0.209877</td>\n",
       "      <td>0.183083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>-0.010115</td>\n",
       "      <td>-0.086336</td>\n",
       "      <td>0.236420</td>\n",
       "      <td>-0.076623</td>\n",
       "      <td>-0.001884</td>\n",
       "      <td>0.043003</td>\n",
       "      <td>-0.074909</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.076582</td>\n",
       "      <td>-0.046084</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013426</td>\n",
       "      <td>-0.086819</td>\n",
       "      <td>-0.065142</td>\n",
       "      <td>0.103260</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.093840</td>\n",
       "      <td>-0.021102</td>\n",
       "      <td>-0.068686</td>\n",
       "      <td>-0.061535</td>\n",
       "      <td>-0.010505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>-0.004343</td>\n",
       "      <td>0.159760</td>\n",
       "      <td>-0.024922</td>\n",
       "      <td>-0.085216</td>\n",
       "      <td>-0.049034</td>\n",
       "      <td>-0.008890</td>\n",
       "      <td>-0.009843</td>\n",
       "      <td>-0.160252</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>-0.089855</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259391</td>\n",
       "      <td>0.137201</td>\n",
       "      <td>-0.073978</td>\n",
       "      <td>0.013475</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.064603</td>\n",
       "      <td>0.103293</td>\n",
       "      <td>0.096905</td>\n",
       "      <td>0.148357</td>\n",
       "      <td>-0.007401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>-0.169782</td>\n",
       "      <td>-0.230794</td>\n",
       "      <td>0.124802</td>\n",
       "      <td>-0.105332</td>\n",
       "      <td>0.037831</td>\n",
       "      <td>-0.025754</td>\n",
       "      <td>-0.009498</td>\n",
       "      <td>-0.050969</td>\n",
       "      <td>-0.001121</td>\n",
       "      <td>0.204646</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.266167</td>\n",
       "      <td>0.510709</td>\n",
       "      <td>0.055459</td>\n",
       "      <td>-0.047183</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007338</td>\n",
       "      <td>0.136956</td>\n",
       "      <td>0.287746</td>\n",
       "      <td>0.299608</td>\n",
       "      <td>0.095331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>0.018408</td>\n",
       "      <td>0.274211</td>\n",
       "      <td>-0.063062</td>\n",
       "      <td>0.050598</td>\n",
       "      <td>-0.377500</td>\n",
       "      <td>0.062043</td>\n",
       "      <td>-0.022106</td>\n",
       "      <td>-0.149405</td>\n",
       "      <td>-0.008882</td>\n",
       "      <td>0.094392</td>\n",
       "      <td>...</td>\n",
       "      <td>0.076970</td>\n",
       "      <td>-0.058274</td>\n",
       "      <td>0.467987</td>\n",
       "      <td>-0.013005</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.093668</td>\n",
       "      <td>0.136022</td>\n",
       "      <td>0.182355</td>\n",
       "      <td>0.204642</td>\n",
       "      <td>-0.150610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>-0.192470</td>\n",
       "      <td>-0.039792</td>\n",
       "      <td>-0.027573</td>\n",
       "      <td>-0.106569</td>\n",
       "      <td>-0.045498</td>\n",
       "      <td>-0.054311</td>\n",
       "      <td>-0.080653</td>\n",
       "      <td>-0.050527</td>\n",
       "      <td>-0.098408</td>\n",
       "      <td>0.153896</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049994</td>\n",
       "      <td>0.058780</td>\n",
       "      <td>0.052885</td>\n",
       "      <td>0.054068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.093043</td>\n",
       "      <td>-0.030108</td>\n",
       "      <td>0.042828</td>\n",
       "      <td>0.005184</td>\n",
       "      <td>0.030429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>-0.014218</td>\n",
       "      <td>0.030414</td>\n",
       "      <td>-0.001231</td>\n",
       "      <td>0.023161</td>\n",
       "      <td>0.005601</td>\n",
       "      <td>-0.024358</td>\n",
       "      <td>-0.064180</td>\n",
       "      <td>0.012682</td>\n",
       "      <td>-0.002215</td>\n",
       "      <td>-0.029544</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.031653</td>\n",
       "      <td>-0.012381</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.009749</td>\n",
       "      <td>-0.047060</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>-0.031406</td>\n",
       "      <td>-0.047340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>-0.048532</td>\n",
       "      <td>-0.035451</td>\n",
       "      <td>0.046644</td>\n",
       "      <td>-0.072899</td>\n",
       "      <td>-0.057896</td>\n",
       "      <td>0.041184</td>\n",
       "      <td>0.013333</td>\n",
       "      <td>-0.003367</td>\n",
       "      <td>0.154739</td>\n",
       "      <td>0.102102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.147928</td>\n",
       "      <td>0.006046</td>\n",
       "      <td>0.025644</td>\n",
       "      <td>-0.005805</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.727015</td>\n",
       "      <td>-0.010316</td>\n",
       "      <td>-0.002547</td>\n",
       "      <td>-0.005308</td>\n",
       "      <td>-0.086873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>-0.154652</td>\n",
       "      <td>-0.125944</td>\n",
       "      <td>0.051123</td>\n",
       "      <td>-0.047675</td>\n",
       "      <td>0.088294</td>\n",
       "      <td>0.013808</td>\n",
       "      <td>0.075032</td>\n",
       "      <td>0.075201</td>\n",
       "      <td>-0.048473</td>\n",
       "      <td>0.068330</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079007</td>\n",
       "      <td>0.117444</td>\n",
       "      <td>0.152438</td>\n",
       "      <td>0.010661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.189453</td>\n",
       "      <td>0.501314</td>\n",
       "      <td>0.175287</td>\n",
       "      <td>0.516590</td>\n",
       "      <td>0.108923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>-0.154598</td>\n",
       "      <td>0.084277</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>-0.052980</td>\n",
       "      <td>-0.358333</td>\n",
       "      <td>0.011963</td>\n",
       "      <td>-0.072962</td>\n",
       "      <td>-0.218323</td>\n",
       "      <td>-0.016159</td>\n",
       "      <td>0.223323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.045497</td>\n",
       "      <td>0.388634</td>\n",
       "      <td>0.358961</td>\n",
       "      <td>-0.012150</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.065948</td>\n",
       "      <td>0.246310</td>\n",
       "      <td>0.367786</td>\n",
       "      <td>0.431126</td>\n",
       "      <td>-0.083396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>-0.217505</td>\n",
       "      <td>-0.039045</td>\n",
       "      <td>0.022031</td>\n",
       "      <td>-0.064111</td>\n",
       "      <td>-0.139293</td>\n",
       "      <td>0.005911</td>\n",
       "      <td>0.040346</td>\n",
       "      <td>-0.031375</td>\n",
       "      <td>-0.056822</td>\n",
       "      <td>0.199613</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.089758</td>\n",
       "      <td>0.372740</td>\n",
       "      <td>0.371309</td>\n",
       "      <td>-0.003645</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.182085</td>\n",
       "      <td>0.506839</td>\n",
       "      <td>0.418380</td>\n",
       "      <td>0.691594</td>\n",
       "      <td>0.034138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-0.219259</td>\n",
       "      <td>-0.049708</td>\n",
       "      <td>0.120165</td>\n",
       "      <td>-0.101982</td>\n",
       "      <td>-0.023104</td>\n",
       "      <td>0.122850</td>\n",
       "      <td>-0.087775</td>\n",
       "      <td>-0.016192</td>\n",
       "      <td>0.056260</td>\n",
       "      <td>-0.086307</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.032726</td>\n",
       "      <td>-0.392319</td>\n",
       "      <td>-0.204171</td>\n",
       "      <td>0.032357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.040094</td>\n",
       "      <td>0.463171</td>\n",
       "      <td>-0.449177</td>\n",
       "      <td>0.053849</td>\n",
       "      <td>0.020487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>0.057003</td>\n",
       "      <td>0.194834</td>\n",
       "      <td>-0.040735</td>\n",
       "      <td>-0.014402</td>\n",
       "      <td>-0.127433</td>\n",
       "      <td>-0.051190</td>\n",
       "      <td>0.032453</td>\n",
       "      <td>-0.076897</td>\n",
       "      <td>-0.033862</td>\n",
       "      <td>-0.044350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.747330</td>\n",
       "      <td>0.029036</td>\n",
       "      <td>-0.010610</td>\n",
       "      <td>0.022243</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.093102</td>\n",
       "      <td>0.149175</td>\n",
       "      <td>0.118731</td>\n",
       "      <td>0.203613</td>\n",
       "      <td>-0.162153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-0.177121</td>\n",
       "      <td>-0.203435</td>\n",
       "      <td>0.050327</td>\n",
       "      <td>-0.077160</td>\n",
       "      <td>0.070543</td>\n",
       "      <td>-0.001358</td>\n",
       "      <td>0.068302</td>\n",
       "      <td>0.024399</td>\n",
       "      <td>-0.015215</td>\n",
       "      <td>0.254764</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.272623</td>\n",
       "      <td>0.879162</td>\n",
       "      <td>-0.011459</td>\n",
       "      <td>-0.066412</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.042415</td>\n",
       "      <td>0.105693</td>\n",
       "      <td>0.508926</td>\n",
       "      <td>0.448430</td>\n",
       "      <td>0.079669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>0.030463</td>\n",
       "      <td>0.215832</td>\n",
       "      <td>-0.098979</td>\n",
       "      <td>0.013548</td>\n",
       "      <td>-0.279070</td>\n",
       "      <td>-0.042551</td>\n",
       "      <td>0.062253</td>\n",
       "      <td>-0.117925</td>\n",
       "      <td>-0.069844</td>\n",
       "      <td>0.228884</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081426</td>\n",
       "      <td>-0.052481</td>\n",
       "      <td>0.810099</td>\n",
       "      <td>-0.043866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.123366</td>\n",
       "      <td>0.002916</td>\n",
       "      <td>0.445921</td>\n",
       "      <td>0.297362</td>\n",
       "      <td>-0.068410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>0.080926</td>\n",
       "      <td>0.048631</td>\n",
       "      <td>-0.009260</td>\n",
       "      <td>-0.021068</td>\n",
       "      <td>0.214977</td>\n",
       "      <td>0.000599</td>\n",
       "      <td>0.092945</td>\n",
       "      <td>0.096299</td>\n",
       "      <td>0.022300</td>\n",
       "      <td>0.023395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023430</td>\n",
       "      <td>0.032085</td>\n",
       "      <td>0.032931</td>\n",
       "      <td>0.027217</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.053710</td>\n",
       "      <td>-0.162693</td>\n",
       "      <td>0.235625</td>\n",
       "      <td>0.031749</td>\n",
       "      <td>0.053808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>-0.049873</td>\n",
       "      <td>0.004522</td>\n",
       "      <td>0.035679</td>\n",
       "      <td>-0.064991</td>\n",
       "      <td>-0.081210</td>\n",
       "      <td>0.078154</td>\n",
       "      <td>-0.020538</td>\n",
       "      <td>-0.001012</td>\n",
       "      <td>0.214108</td>\n",
       "      <td>0.076263</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.115926</td>\n",
       "      <td>-0.011274</td>\n",
       "      <td>-0.092968</td>\n",
       "      <td>-0.016002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.898566</td>\n",
       "      <td>-0.013811</td>\n",
       "      <td>-0.073057</td>\n",
       "      <td>-0.046688</td>\n",
       "      <td>-0.081166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>-0.256627</td>\n",
       "      <td>-0.027394</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>-0.126629</td>\n",
       "      <td>-0.055768</td>\n",
       "      <td>0.067850</td>\n",
       "      <td>0.015444</td>\n",
       "      <td>-0.043371</td>\n",
       "      <td>-0.013899</td>\n",
       "      <td>0.092021</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015869</td>\n",
       "      <td>0.096861</td>\n",
       "      <td>0.032699</td>\n",
       "      <td>-0.008024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.121979</td>\n",
       "      <td>0.864417</td>\n",
       "      <td>0.003682</td>\n",
       "      <td>0.676655</td>\n",
       "      <td>0.062174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>-0.072094</td>\n",
       "      <td>0.021588</td>\n",
       "      <td>-0.034349</td>\n",
       "      <td>-0.017201</td>\n",
       "      <td>-0.077276</td>\n",
       "      <td>-0.033589</td>\n",
       "      <td>0.131198</td>\n",
       "      <td>-0.032750</td>\n",
       "      <td>-0.052589</td>\n",
       "      <td>0.307150</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.007387</td>\n",
       "      <td>0.665484</td>\n",
       "      <td>0.474834</td>\n",
       "      <td>-0.044152</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.129309</td>\n",
       "      <td>0.026729</td>\n",
       "      <td>0.813504</td>\n",
       "      <td>0.583441</td>\n",
       "      <td>-0.010580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>-0.227229</td>\n",
       "      <td>-0.024504</td>\n",
       "      <td>-0.042376</td>\n",
       "      <td>-0.091399</td>\n",
       "      <td>-0.061721</td>\n",
       "      <td>0.011672</td>\n",
       "      <td>0.128242</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>-0.046543</td>\n",
       "      <td>0.258024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018052</td>\n",
       "      <td>0.511562</td>\n",
       "      <td>0.336019</td>\n",
       "      <td>-0.033059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.158144</td>\n",
       "      <td>0.610243</td>\n",
       "      <td>0.549813</td>\n",
       "      <td>0.881998</td>\n",
       "      <td>0.052617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>-0.219194</td>\n",
       "      <td>-0.013236</td>\n",
       "      <td>0.130048</td>\n",
       "      <td>-0.124160</td>\n",
       "      <td>-0.258353</td>\n",
       "      <td>0.108287</td>\n",
       "      <td>-0.191445</td>\n",
       "      <td>-0.182234</td>\n",
       "      <td>0.046601</td>\n",
       "      <td>-0.093831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009015</td>\n",
       "      <td>-0.377066</td>\n",
       "      <td>-0.263369</td>\n",
       "      <td>0.042103</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.033788</td>\n",
       "      <td>0.540750</td>\n",
       "      <td>-0.580298</td>\n",
       "      <td>0.011432</td>\n",
       "      <td>-0.062129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>0.164816</td>\n",
       "      <td>0.230938</td>\n",
       "      <td>-0.018876</td>\n",
       "      <td>0.050471</td>\n",
       "      <td>-0.198941</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>0.035215</td>\n",
       "      <td>-0.068035</td>\n",
       "      <td>-0.010142</td>\n",
       "      <td>-0.179725</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.192113</td>\n",
       "      <td>-0.077984</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.085446</td>\n",
       "      <td>0.116959</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.082538</td>\n",
       "      <td>-0.164321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>-0.158009</td>\n",
       "      <td>-0.042638</td>\n",
       "      <td>-0.073439</td>\n",
       "      <td>-0.026926</td>\n",
       "      <td>0.091384</td>\n",
       "      <td>0.003411</td>\n",
       "      <td>0.121349</td>\n",
       "      <td>0.070535</td>\n",
       "      <td>0.007886</td>\n",
       "      <td>0.292155</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.192113</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>-0.090275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.008013</td>\n",
       "      <td>0.093749</td>\n",
       "      <td>0.671383</td>\n",
       "      <td>0.560374</td>\n",
       "      <td>0.036188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>0.082376</td>\n",
       "      <td>0.092879</td>\n",
       "      <td>-0.091361</td>\n",
       "      <td>0.051946</td>\n",
       "      <td>-0.228688</td>\n",
       "      <td>-0.074180</td>\n",
       "      <td>0.116718</td>\n",
       "      <td>-0.046868</td>\n",
       "      <td>-0.091258</td>\n",
       "      <td>0.295283</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077984</td>\n",
       "      <td>0.016345</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.011419</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.092069</td>\n",
       "      <td>-0.072408</td>\n",
       "      <td>0.562825</td>\n",
       "      <td>0.341484</td>\n",
       "      <td>-0.071556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>0.090413</td>\n",
       "      <td>0.027401</td>\n",
       "      <td>-0.002545</td>\n",
       "      <td>0.047448</td>\n",
       "      <td>-0.013697</td>\n",
       "      <td>0.068594</td>\n",
       "      <td>-0.002351</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>0.096772</td>\n",
       "      <td>-0.071495</td>\n",
       "      <td>...</td>\n",
       "      <td>0.040477</td>\n",
       "      <td>-0.090275</td>\n",
       "      <td>-0.011419</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.019946</td>\n",
       "      <td>-0.017690</td>\n",
       "      <td>-0.051131</td>\n",
       "      <td>-0.049667</td>\n",
       "      <td>-0.036992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>-0.037877</td>\n",
       "      <td>0.014210</td>\n",
       "      <td>0.067670</td>\n",
       "      <td>-0.046278</td>\n",
       "      <td>-0.065596</td>\n",
       "      <td>0.143272</td>\n",
       "      <td>-0.034971</td>\n",
       "      <td>0.049175</td>\n",
       "      <td>0.248092</td>\n",
       "      <td>0.082009</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.085446</td>\n",
       "      <td>-0.008013</td>\n",
       "      <td>-0.092069</td>\n",
       "      <td>-0.019946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>-0.069852</td>\n",
       "      <td>-0.064171</td>\n",
       "      <td>-0.086427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>-0.271504</td>\n",
       "      <td>0.065684</td>\n",
       "      <td>-0.008471</td>\n",
       "      <td>-0.144721</td>\n",
       "      <td>-0.222170</td>\n",
       "      <td>0.061160</td>\n",
       "      <td>-0.038884</td>\n",
       "      <td>-0.184846</td>\n",
       "      <td>0.016054</td>\n",
       "      <td>0.063300</td>\n",
       "      <td>...</td>\n",
       "      <td>0.116959</td>\n",
       "      <td>0.093749</td>\n",
       "      <td>-0.072408</td>\n",
       "      <td>-0.017690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.043307</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124004</td>\n",
       "      <td>0.687877</td>\n",
       "      <td>-0.030798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.018043</td>\n",
       "      <td>0.031688</td>\n",
       "      <td>-0.090370</td>\n",
       "      <td>0.062285</td>\n",
       "      <td>0.129796</td>\n",
       "      <td>-0.027691</td>\n",
       "      <td>0.256200</td>\n",
       "      <td>0.130202</td>\n",
       "      <td>-0.016365</td>\n",
       "      <td>0.297780</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.671383</td>\n",
       "      <td>0.562825</td>\n",
       "      <td>-0.051131</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.069852</td>\n",
       "      <td>-0.124004</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.611219</td>\n",
       "      <td>0.003476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>-0.199728</td>\n",
       "      <td>0.046605</td>\n",
       "      <td>-0.092235</td>\n",
       "      <td>-0.050682</td>\n",
       "      <td>-0.082791</td>\n",
       "      <td>0.021048</td>\n",
       "      <td>0.150979</td>\n",
       "      <td>-0.014430</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>0.255203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082538</td>\n",
       "      <td>0.560374</td>\n",
       "      <td>0.341484</td>\n",
       "      <td>-0.049667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.064171</td>\n",
       "      <td>0.687877</td>\n",
       "      <td>0.611219</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>-0.092381</td>\n",
       "      <td>-0.178080</td>\n",
       "      <td>0.006648</td>\n",
       "      <td>-0.090151</td>\n",
       "      <td>0.323879</td>\n",
       "      <td>-0.099954</td>\n",
       "      <td>0.028305</td>\n",
       "      <td>0.097625</td>\n",
       "      <td>-0.122003</td>\n",
       "      <td>0.019585</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.164321</td>\n",
       "      <td>0.036188</td>\n",
       "      <td>-0.071556</td>\n",
       "      <td>-0.036992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.086427</td>\n",
       "      <td>-0.030798</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>-0.009275</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>279 rows Ã— 279 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6    \\\n",
       "0    1.000000 -0.059042 -0.109458  0.381555 -0.004032  0.041149  0.195691   \n",
       "1   -0.059042  1.000000 -0.124685 -0.248104 -0.337101 -0.046771  0.072052   \n",
       "2   -0.109458 -0.124685  1.000000 -0.074957 -0.006329  0.013601 -0.237314   \n",
       "3    0.381555 -0.248104 -0.074957  1.000000  0.100094  0.119826  0.118657   \n",
       "4   -0.004032 -0.337101 -0.006329  0.100094  1.000000  0.021831  0.218681   \n",
       "5    0.041149 -0.046771  0.013601  0.119826  0.021831  1.000000  0.079371   \n",
       "6    0.195691  0.072052 -0.237314  0.118657  0.218681  0.079371  1.000000   \n",
       "7    0.025654 -0.184736 -0.038411  0.149987  0.397435  0.074618  0.166711   \n",
       "8    0.099755 -0.081051  0.029025  0.120668  0.049682  0.670865  0.063044   \n",
       "9   -0.265868  0.069434  0.061539 -0.173355 -0.146043 -0.012412 -0.031786   \n",
       "10   0.008095 -0.141667  0.000108 -0.046153  0.031114  0.114645  0.159971   \n",
       "11  -0.046294  0.013971 -0.106477 -0.026709  0.040207  0.008069 -0.091543   \n",
       "12  -0.285805  0.030865  0.024969 -0.212601 -0.075626  0.020685 -0.084250   \n",
       "13  -0.192239  0.062729  0.286553 -0.174988 -0.006001 -0.045091 -0.654681   \n",
       "14  -0.016675 -0.129383 -0.001940  0.038332  0.118847  0.017437  0.018153   \n",
       "15   0.199533  0.016725 -0.091314  0.117731  0.310412 -0.067438  0.296504   \n",
       "16  -0.120405 -0.149390  0.040893 -0.023805  0.185647  0.053189 -0.084368   \n",
       "17   0.028642 -0.020588  0.006688  0.119989  0.051225  0.012168  0.014332   \n",
       "18        NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "19   0.152401 -0.088119 -0.082112  0.145418  0.415102 -0.034179  0.233446   \n",
       "20   0.044455  0.042517  0.004834  0.118853  0.024790 -0.163104 -0.103370   \n",
       "21   0.130713  0.010443 -0.014208 -0.002364  0.196260  0.054418  0.076723   \n",
       "22   0.008544 -0.074608  0.014277  0.001466  0.000549  0.010501  0.080529   \n",
       "23   0.089284 -0.006820 -0.010213 -0.010742  0.022065 -0.109620  0.012578   \n",
       "24   0.056860 -0.073835  0.003253  0.009372  0.165412  0.005726  0.091543   \n",
       "25   0.031761 -0.104651  0.006520  0.070300  0.243685  0.004952  0.083706   \n",
       "26  -0.039709 -0.139389 -0.013874  0.086654  0.093043  0.000195 -0.061630   \n",
       "27   0.206806  0.016074 -0.064018  0.131662  0.261717 -0.003799  0.253075   \n",
       "28  -0.130212 -0.117475 -0.010917 -0.055522  0.292268 -0.027114  0.055224   \n",
       "29  -0.010115 -0.086336  0.236420 -0.076623 -0.001884  0.043003 -0.074909   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "249 -0.004343  0.159760 -0.024922 -0.085216 -0.049034 -0.008890 -0.009843   \n",
       "250 -0.169782 -0.230794  0.124802 -0.105332  0.037831 -0.025754 -0.009498   \n",
       "251  0.018408  0.274211 -0.063062  0.050598 -0.377500  0.062043 -0.022106   \n",
       "252 -0.192470 -0.039792 -0.027573 -0.106569 -0.045498 -0.054311 -0.080653   \n",
       "253 -0.014218  0.030414 -0.001231  0.023161  0.005601 -0.024358 -0.064180   \n",
       "254 -0.048532 -0.035451  0.046644 -0.072899 -0.057896  0.041184  0.013333   \n",
       "255 -0.154652 -0.125944  0.051123 -0.047675  0.088294  0.013808  0.075032   \n",
       "256 -0.154598  0.084277  0.012468 -0.052980 -0.358333  0.011963 -0.072962   \n",
       "257 -0.217505 -0.039045  0.022031 -0.064111 -0.139293  0.005911  0.040346   \n",
       "258 -0.219259 -0.049708  0.120165 -0.101982 -0.023104  0.122850 -0.087775   \n",
       "259  0.057003  0.194834 -0.040735 -0.014402 -0.127433 -0.051190  0.032453   \n",
       "260 -0.177121 -0.203435  0.050327 -0.077160  0.070543 -0.001358  0.068302   \n",
       "261  0.030463  0.215832 -0.098979  0.013548 -0.279070 -0.042551  0.062253   \n",
       "262  0.080926  0.048631 -0.009260 -0.021068  0.214977  0.000599  0.092945   \n",
       "263       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "264 -0.049873  0.004522  0.035679 -0.064991 -0.081210  0.078154 -0.020538   \n",
       "265 -0.256627 -0.027394  0.006517 -0.126629 -0.055768  0.067850  0.015444   \n",
       "266 -0.072094  0.021588 -0.034349 -0.017201 -0.077276 -0.033589  0.131198   \n",
       "267 -0.227229 -0.024504 -0.042376 -0.091399 -0.061721  0.011672  0.128242   \n",
       "268 -0.219194 -0.013236  0.130048 -0.124160 -0.258353  0.108287 -0.191445   \n",
       "269  0.164816  0.230938 -0.018876  0.050471 -0.198941 -0.005347  0.035215   \n",
       "270 -0.158009 -0.042638 -0.073439 -0.026926  0.091384  0.003411  0.121349   \n",
       "271  0.082376  0.092879 -0.091361  0.051946 -0.228688 -0.074180  0.116718   \n",
       "272  0.090413  0.027401 -0.002545  0.047448 -0.013697  0.068594 -0.002351   \n",
       "273       NaN       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "274 -0.037877  0.014210  0.067670 -0.046278 -0.065596  0.143272 -0.034971   \n",
       "275 -0.271504  0.065684 -0.008471 -0.144721 -0.222170  0.061160 -0.038884   \n",
       "276  0.018043  0.031688 -0.090370  0.062285  0.129796 -0.027691  0.256200   \n",
       "277 -0.199728  0.046605 -0.092235 -0.050682 -0.082791  0.021048  0.150979   \n",
       "278 -0.092381 -0.178080  0.006648 -0.090151  0.323879 -0.099954  0.028305   \n",
       "\n",
       "          7         8         9      ...          269       270       271  \\\n",
       "0    0.025654  0.099755 -0.265868    ...     0.164816 -0.158009  0.082376   \n",
       "1   -0.184736 -0.081051  0.069434    ...     0.230938 -0.042638  0.092879   \n",
       "2   -0.038411  0.029025  0.061539    ...    -0.018876 -0.073439 -0.091361   \n",
       "3    0.149987  0.120668 -0.173355    ...     0.050471 -0.026926  0.051946   \n",
       "4    0.397435  0.049682 -0.146043    ...    -0.198941  0.091384 -0.228688   \n",
       "5    0.074618  0.670865 -0.012412    ...    -0.005347  0.003411 -0.074180   \n",
       "6    0.166711  0.063044 -0.031786    ...     0.035215  0.121349  0.116718   \n",
       "7    1.000000  0.060302 -0.099283    ...    -0.068035  0.070535 -0.046868   \n",
       "8    0.060302  1.000000 -0.062760    ...    -0.010142  0.007886 -0.091258   \n",
       "9   -0.099283 -0.062760  1.000000    ...    -0.179725  0.292155  0.295283   \n",
       "10   0.040838  0.143195 -0.051925    ...    -0.034564 -0.052905 -0.055454   \n",
       "11  -0.111296  0.054635  0.054649    ...    -0.007202 -0.065499  0.063194   \n",
       "12  -0.107960 -0.073149  0.709528    ...    -0.222315  0.169013  0.132601   \n",
       "13   0.016251  0.043433  0.011351    ...    -0.051391 -0.118857 -0.236425   \n",
       "14   0.024361  0.072590 -0.055314    ...    -0.342547 -0.118037 -0.028179   \n",
       "15   0.251603 -0.025343 -0.271659    ...     0.195220  0.126898  0.199933   \n",
       "16  -0.037765  0.035735  0.213609    ...    -0.112477  0.025226 -0.302136   \n",
       "17   0.013292 -0.011175  0.055497    ...    -0.037438  0.034874  0.040968   \n",
       "18        NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
       "19   0.287783  0.008511 -0.258226    ...    -0.053776  0.126330  0.039008   \n",
       "20   0.018588 -0.164281  0.005523    ...     0.023960  0.084864  0.034307   \n",
       "21   0.072653  0.075415  0.006812    ...     0.053816 -0.024634  0.022065   \n",
       "22  -0.022431 -0.071352 -0.070562    ...    -0.038777 -0.079512  0.001972   \n",
       "23  -0.032682  0.052964 -0.060491    ...     0.033923 -0.057615  0.025241   \n",
       "24   0.132141 -0.010348 -0.064163    ...     0.033923  0.016373 -0.019755   \n",
       "25   0.200566  0.014635  0.045439    ...    -0.016548  0.019802  0.038137   \n",
       "26   0.005486 -0.000912  0.126140    ...    -0.498626  0.073974  0.100968   \n",
       "27   0.151196 -0.044184  0.306674    ...     0.074808  0.225314  0.297373   \n",
       "28   0.160626  0.089904 -0.433352    ...     0.059455 -0.140348 -0.404699   \n",
       "29   0.001327  0.076582 -0.046084    ...     0.013426 -0.086819 -0.065142   \n",
       "..        ...       ...       ...    ...          ...       ...       ...   \n",
       "249 -0.160252 -0.000557 -0.089855    ...     0.259391  0.137201 -0.073978   \n",
       "250 -0.050969 -0.001121  0.204646    ...    -0.266167  0.510709  0.055459   \n",
       "251 -0.149405 -0.008882  0.094392    ...     0.076970 -0.058274  0.467987   \n",
       "252 -0.050527 -0.098408  0.153896    ...    -0.049994  0.058780  0.052885   \n",
       "253  0.012682 -0.002215 -0.029544    ...    -0.031653 -0.012381  0.002524   \n",
       "254 -0.003367  0.154739  0.102102    ...    -0.147928  0.006046  0.025644   \n",
       "255  0.075201 -0.048473  0.068330    ...    -0.079007  0.117444  0.152438   \n",
       "256 -0.218323 -0.016159  0.223323    ...    -0.045497  0.388634  0.358961   \n",
       "257 -0.031375 -0.056822  0.199613    ...    -0.089758  0.372740  0.371309   \n",
       "258 -0.016192  0.056260 -0.086307    ...    -0.032726 -0.392319 -0.204171   \n",
       "259 -0.076897 -0.033862 -0.044350    ...     0.747330  0.029036 -0.010610   \n",
       "260  0.024399 -0.015215  0.254764    ...    -0.272623  0.879162 -0.011459   \n",
       "261 -0.117925 -0.069844  0.228884    ...    -0.081426 -0.052481  0.810099   \n",
       "262  0.096299  0.022300  0.023395    ...     0.023430  0.032085  0.032931   \n",
       "263       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
       "264 -0.001012  0.214108  0.076263    ...    -0.115926 -0.011274 -0.092968   \n",
       "265 -0.043371 -0.013899  0.092021    ...    -0.015869  0.096861  0.032699   \n",
       "266 -0.032750 -0.052589  0.307150    ...    -0.007387  0.665484  0.474834   \n",
       "267  0.005624 -0.046543  0.258024    ...    -0.018052  0.511562  0.336019   \n",
       "268 -0.182234  0.046601 -0.093831    ...    -0.009015 -0.377066 -0.263369   \n",
       "269 -0.068035 -0.010142 -0.179725    ...     1.000000 -0.192113 -0.077984   \n",
       "270  0.070535  0.007886  0.292155    ...    -0.192113  1.000000  0.016345   \n",
       "271 -0.046868 -0.091258  0.295283    ...    -0.077984  0.016345  1.000000   \n",
       "272  0.008556  0.096772 -0.071495    ...     0.040477 -0.090275 -0.011419   \n",
       "273       NaN       NaN       NaN    ...          NaN       NaN       NaN   \n",
       "274  0.049175  0.248092  0.082009    ...    -0.085446 -0.008013 -0.092069   \n",
       "275 -0.184846  0.016054  0.063300    ...     0.116959  0.093749 -0.072408   \n",
       "276  0.130202 -0.016365  0.297780    ...     0.002630  0.671383  0.562825   \n",
       "277 -0.014430  0.004283  0.255203    ...     0.082538  0.560374  0.341484   \n",
       "278  0.097625 -0.122003  0.019585    ...    -0.164321  0.036188 -0.071556   \n",
       "\n",
       "          272  273       274       275       276       277       278  \n",
       "0    0.090413  NaN -0.037877 -0.271504  0.018043 -0.199728 -0.092381  \n",
       "1    0.027401  NaN  0.014210  0.065684  0.031688  0.046605 -0.178080  \n",
       "2   -0.002545  NaN  0.067670 -0.008471 -0.090370 -0.092235  0.006648  \n",
       "3    0.047448  NaN -0.046278 -0.144721  0.062285 -0.050682 -0.090151  \n",
       "4   -0.013697  NaN -0.065596 -0.222170  0.129796 -0.082791  0.323879  \n",
       "5    0.068594  NaN  0.143272  0.061160 -0.027691  0.021048 -0.099954  \n",
       "6   -0.002351  NaN -0.034971 -0.038884  0.256200  0.150979  0.028305  \n",
       "7    0.008556  NaN  0.049175 -0.184846  0.130202 -0.014430  0.097625  \n",
       "8    0.096772  NaN  0.248092  0.016054 -0.016365  0.004283 -0.122003  \n",
       "9   -0.071495  NaN  0.082009  0.063300  0.297780  0.255203  0.019585  \n",
       "10   0.004383  NaN  0.071090  0.157539 -0.062546  0.084336  0.005178  \n",
       "11  -0.003007  NaN  0.212522  0.039697 -0.036134 -0.013432 -0.012897  \n",
       "12  -0.066043  NaN  0.052213  0.194113  0.101686  0.214848  0.069403  \n",
       "13   0.032815  NaN  0.179483 -0.083605 -0.249300 -0.234168  0.008331  \n",
       "14  -0.042085  NaN  0.086201 -0.037790 -0.170610 -0.163450  0.003965  \n",
       "15   0.094711  NaN -0.103729 -0.241676  0.484570  0.159676  0.042674  \n",
       "16  -0.081102  NaN  0.008272  0.099483 -0.267412 -0.102082  0.195198  \n",
       "17  -0.007184  NaN -0.008735  0.040192  0.060671  0.073527  0.063999  \n",
       "18        NaN  NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "19  -0.007327  NaN -0.003834 -0.198374  0.309029  0.072924  0.042764  \n",
       "20  -0.003746  NaN -0.137654 -0.033795  0.060650  0.024028 -0.020115  \n",
       "21   0.329603  NaN -0.065449 -0.136788  0.147827 -0.019770  0.007675  \n",
       "22  -0.008413  NaN  0.001577 -0.066996 -0.079552 -0.118700 -0.035569  \n",
       "23  -0.005303  NaN -0.079663 -0.008066 -0.030277 -0.028413  0.032097  \n",
       "24  -0.005303  NaN -0.050857 -0.087627  0.117004  0.015253  0.032097  \n",
       "25  -0.007516  NaN -0.038059 -0.119225  0.159007  0.012541  0.083056  \n",
       "26   0.007432  NaN  0.098454 -0.102425  0.006019 -0.063413 -0.003612  \n",
       "27  -0.011813  NaN  0.000868 -0.140302  0.528282  0.255147  0.048367  \n",
       "28  -0.016526  NaN -0.053551  0.017808 -0.319503 -0.209877  0.183083  \n",
       "29   0.103260  NaN  0.093840 -0.021102 -0.068686 -0.061535 -0.010505  \n",
       "..        ...  ...       ...       ...       ...       ...       ...  \n",
       "249  0.013475  NaN -0.064603  0.103293  0.096905  0.148357 -0.007401  \n",
       "250 -0.047183  NaN  0.007338  0.136956  0.287746  0.299608  0.095331  \n",
       "251 -0.013005  NaN -0.093668  0.136022  0.182355  0.204642 -0.150610  \n",
       "252  0.054068  NaN -0.093043 -0.030108  0.042828  0.005184  0.030429  \n",
       "253  0.004948  NaN -0.009749 -0.047060  0.008386 -0.031406 -0.047340  \n",
       "254 -0.005805  NaN  0.727015 -0.010316 -0.002547 -0.005308 -0.086873  \n",
       "255  0.010661  NaN -0.189453  0.501314  0.175287  0.516590  0.108923  \n",
       "256 -0.012150  NaN -0.065948  0.246310  0.367786  0.431126 -0.083396  \n",
       "257 -0.003645  NaN -0.182085  0.506839  0.418380  0.691594  0.034138  \n",
       "258  0.032357  NaN  0.040094  0.463171 -0.449177  0.053849  0.020487  \n",
       "259  0.022243  NaN -0.093102  0.149175  0.118731  0.203613 -0.162153  \n",
       "260 -0.066412  NaN -0.042415  0.105693  0.508926  0.448430  0.079669  \n",
       "261 -0.043866  NaN -0.123366  0.002916  0.445921  0.297362 -0.068410  \n",
       "262  0.027217  NaN -0.053710 -0.162693  0.235625  0.031749  0.053808  \n",
       "263       NaN  NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "264 -0.016002  NaN  0.898566 -0.013811 -0.073057 -0.046688 -0.081166  \n",
       "265 -0.008024  NaN -0.121979  0.864417  0.003682  0.676655  0.062174  \n",
       "266 -0.044152  NaN -0.129309  0.026729  0.813504  0.583441 -0.010580  \n",
       "267 -0.033059  NaN -0.158144  0.610243  0.549813  0.881998  0.052617  \n",
       "268  0.042103  NaN  0.033788  0.540750 -0.580298  0.011432 -0.062129  \n",
       "269  0.040477  NaN -0.085446  0.116959  0.002630  0.082538 -0.164321  \n",
       "270 -0.090275  NaN -0.008013  0.093749  0.671383  0.560374  0.036188  \n",
       "271 -0.011419  NaN -0.092069 -0.072408  0.562825  0.341484 -0.071556  \n",
       "272  1.000000  NaN -0.019946 -0.017690 -0.051131 -0.049667 -0.036992  \n",
       "273       NaN  NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "274 -0.019946  NaN  1.000000 -0.043307 -0.069852 -0.064171 -0.086427  \n",
       "275 -0.017690  NaN -0.043307  1.000000 -0.124004  0.687877 -0.030798  \n",
       "276 -0.051131  NaN -0.069852 -0.124004  1.000000  0.611219  0.003476  \n",
       "277 -0.049667  NaN -0.064171  0.687877  0.611219  1.000000 -0.009275  \n",
       "278 -0.036992  NaN -0.086427 -0.030798  0.003476 -0.009275  1.000000  \n",
       "\n",
       "[279 rows x 279 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_no_missing.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see there is no major correlation between variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X, y split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>268</th>\n",
       "      <th>269</th>\n",
       "      <th>270</th>\n",
       "      <th>271</th>\n",
       "      <th>272</th>\n",
       "      <th>273</th>\n",
       "      <th>274</th>\n",
       "      <th>275</th>\n",
       "      <th>276</th>\n",
       "      <th>277</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1      2     3      4      5      6      7      8     9    ...   268  \\\n",
       "0  75.0  0.0  190.0  80.0   91.0  193.0  371.0  174.0  121.0 -16.0  ...  -0.3   \n",
       "1  56.0  1.0  165.0  64.0   81.0  174.0  401.0  149.0   39.0  25.0  ...  -0.5   \n",
       "2  54.0  0.0  172.0  95.0  138.0  163.0  386.0  185.0  102.0  96.0  ...   0.9   \n",
       "3  55.0  0.0  175.0  94.0  100.0  202.0  380.0  179.0  143.0  28.0  ...   0.1   \n",
       "4  75.0  0.0  190.0  80.0   88.0  181.0  360.0  177.0  103.0 -16.0  ...  -0.4   \n",
       "\n",
       "   269   270  271  272  273  274  275   276   277  \n",
       "0  0.0   9.0 -0.9  0.0  0.0  0.9  2.9  23.3  49.4  \n",
       "1  0.0   8.5  0.0  0.0  0.0  0.2  2.1  20.4  38.8  \n",
       "2  0.0   9.5 -2.4  0.0  0.0  0.3  3.4  12.3  49.0  \n",
       "3  0.0  12.2 -2.2  0.0  0.0  0.4  2.6  34.6  61.6  \n",
       "4  0.0  13.1 -3.6  0.0  0.0 -0.1  3.9  25.4  62.8  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data_no_missing.drop(columns=278)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_columns=[\"Age\",\"Gender_Nom\",\"Height\",\"Weight\",\"QRS_Dur\",\n",
    "\"P-R_Int\",\"Q-T_Int\",\"T_Int\",\"P_Int\",\"QRS\",\"T\",\"P\",\"J\",\"Heart_Rate\",\n",
    "\"Q_Wave\",\"R_Wave\",\"S_Wave\",\"R_Prime\",\"S_Prime\",\"Int_Def\",\"Rag_R_Nom\",\n",
    "\"Diph_R_Nom\",\"Rag_P_Nom\",\"Diph_P_Nom\",\"Rag_T_Nom\",\"Diph_T_Nom\", \n",
    "\"DII00\", \"DII01\",\"DII02\", \"DII03\", \"DII04\",\"DII05\",\"DII06\",\"DII07\",\"DII08\",\"DII09\",\"DII10\",\"DII11\",\n",
    "\"DIII00\",\"DIII01\",\"DIII02\", \"DIII03\", \"DIII04\",\"DIII05\",\"DIII06\",\"DIII07\",\"DIII08\",\"DIII09\",\"DIII10\",\"DIII11\",\n",
    "\"AVR00\",\"AVR01\",\"AVR02\",\"AVR03\",\"AVR04\",\"AVR05\",\"AVR06\",\"AVR07\",\"AVR08\",\"AVR09\",\"AVR10\",\"AVR11\",\n",
    "\"AVL00\",\"AVRL1\",\"AVL02\",\"AVL03\",\"AVL04\",\"AVL05\",\"AVL06\",\"AVL07\",\"AVL08\",\"AVL09\",\"AVL10\",\"AVL11\",\n",
    "\"AVF00\",\"AVF01\",\"AVF02\",\"AVF03\",\"AVF04\",\"AVF05\",\"AVF06\",\"AVF07\",\"AVF08\",\"AVF09\",\"AVF10\",\"AVF11\",\n",
    "\"V100\",\"V101\",\"V102\",\"V103\",\"V104\",\"V105\",\"V106\",\"V107\",\"V108\",\"V109\",\"V110\",\"V111\",\n",
    "\"V200\",\"V201\",\"V202\",\"V203\",\"V204\",\"V205\",\"V206\",\"V207\",\"V208\",\"V209\",\"V210\",\"V211\",\n",
    "\"V300\",\"V301\",\"V302\",\"V303\",\"V304\",\"V305\",\"V306\",\"V307\",\"V308\",\"V309\",\"V310\",\"V311\",\n",
    "\"V400\",\"V401\",\"V402\",\"V403\",\"V404\",\"V405\",\"V406\",\"V407\",\"V408\",\"V409\",\"V410\",\"V411\",\n",
    "\"V500\",\"V501\",\"V502\",\"V503\",\"V504\",\"V505\",\"V506\",\"V507\",\"V508\",\"V509\",\"V510\",\"V511\",\n",
    "\"V600\",\"V601\",\"V602\",\"V603\",\"V604\",\"V605\",\"V606\",\"V607\",\"V608\",\"V609\",\"V610\",\"V611\",\n",
    "\"JJ_Wave\",\"Q_Wave\",\"R_Wave\",\"S_Wave\",\"R_Prime_Wave\",\"S_Prime_Wave\",\"P_Wave\",\"T_Wave\",\n",
    "\"QRSA\",\"QRSTA\",\"DII170\",\"DII171\",\"DII172\",\"DII173\",\"DII174\",\"DII175\",\"DII176\",\"DII177\",\"DII178\",\"DII179\",\n",
    "\"DIII180\",\"DIII181\",\"DIII182\",\"DIII183\",\"DIII184\",\"DIII185\",\"DIII186\",\"DIII187\",\"DIII188\",\"DIII189\",\n",
    "\"AVR190\",\"AVR191\",\"AVR192\",\"AVR193\",\"AVR194\",\"AVR195\",\"AVR196\",\"AVR197\",\"AVR198\",\"AVR199\",\n",
    "\"AVL200\",\"AVL201\",\"AVL202\",\"AVL203\",\"AVL204\",\"AVL205\",\"AVL206\",\"AVL207\",\"AVL208\",\"AVL209\",\n",
    "\"AVF210\",\"AVF211\",\"AVF212\",\"AVF213\",\"AVF214\",\"AVF215\",\"AVF216\",\"AVF217\",\"AVF218\",\"AVF219\",\n",
    "\"V1220\",\"V1221\",\"V1222\",\"V1223\",\"V1224\",\"V1225\",\"V1226\",\"V1227\",\"V1228\",\"V1229\",\n",
    "\"V2230\",\"V2231\",\"V2232\",\"V2233\",\"V2234\",\"V2235\",\"V2236\",\"V2237\",\"V2238\",\"V2239\",\n",
    "\"V3240\",\"V3241\",\"V3242\",\"V3243\",\"V3244\",\"V3245\",\"V3246\",\"V3247\",\"V3248\",\"V3249\",\n",
    "\"V4250\",\"V4251\",\"V4252\",\"V4253\",\"V4254\",\"V4255\",\"V4256\",\"V4257\",\"V4258\",\"V4259\",\n",
    "\"V5260\",\"V5261\",\"V5262\",\"V5263\",\"V5264\",\"V5265\",\"V5266\",\"V5267\",\"V5268\",\"V5269\",\n",
    "\"V6270\",\"V6271\",\"V6272\",\"V6273\",\"V6274\",\"V6275\",\"V6276\",\"V6277\",\"V6278\",\"V6279\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender_Nom</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>QRS_Dur</th>\n",
       "      <th>P-R_Int</th>\n",
       "      <th>Q-T_Int</th>\n",
       "      <th>T_Int</th>\n",
       "      <th>P_Int</th>\n",
       "      <th>QRS</th>\n",
       "      <th>...</th>\n",
       "      <th>V6270</th>\n",
       "      <th>V6271</th>\n",
       "      <th>V6272</th>\n",
       "      <th>V6273</th>\n",
       "      <th>V6274</th>\n",
       "      <th>V6275</th>\n",
       "      <th>V6276</th>\n",
       "      <th>V6277</th>\n",
       "      <th>V6278</th>\n",
       "      <th>V6279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>121.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>2.9</td>\n",
       "      <td>23.3</td>\n",
       "      <td>49.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>64.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>401.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>20.4</td>\n",
       "      <td>38.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>95.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>386.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.5</td>\n",
       "      <td>-2.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>12.3</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>202.0</td>\n",
       "      <td>380.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>143.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2.6</td>\n",
       "      <td>34.6</td>\n",
       "      <td>61.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>75.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>181.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>-16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>-3.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.9</td>\n",
       "      <td>25.4</td>\n",
       "      <td>62.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age  Gender_Nom  Height  Weight  QRS_Dur  P-R_Int  Q-T_Int  T_Int  P_Int  \\\n",
       "0  75.0         0.0   190.0    80.0     91.0    193.0    371.0  174.0  121.0   \n",
       "1  56.0         1.0   165.0    64.0     81.0    174.0    401.0  149.0   39.0   \n",
       "2  54.0         0.0   172.0    95.0    138.0    163.0    386.0  185.0  102.0   \n",
       "3  55.0         0.0   175.0    94.0    100.0    202.0    380.0  179.0  143.0   \n",
       "4  75.0         0.0   190.0    80.0     88.0    181.0    360.0  177.0  103.0   \n",
       "\n",
       "    QRS  ...    V6270  V6271  V6272  V6273  V6274  V6275  V6276  V6277  V6278  \\\n",
       "0 -16.0  ...     -0.3    0.0    9.0   -0.9    0.0    0.0    0.9    2.9   23.3   \n",
       "1  25.0  ...     -0.5    0.0    8.5    0.0    0.0    0.0    0.2    2.1   20.4   \n",
       "2  96.0  ...      0.9    0.0    9.5   -2.4    0.0    0.0    0.3    3.4   12.3   \n",
       "3  28.0  ...      0.1    0.0   12.2   -2.2    0.0    0.0    0.4    2.6   34.6   \n",
       "4 -16.0  ...     -0.4    0.0   13.1   -3.6    0.0    0.0   -0.1    3.9   25.4   \n",
       "\n",
       "   V6279  \n",
       "0   49.4  \n",
       "1   38.8  \n",
       "2   49.0  \n",
       "3   61.6  \n",
       "4   62.8  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns = X_columns\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8.0\n",
       "1     6.0\n",
       "2    10.0\n",
       "3     1.0\n",
       "4     7.0\n",
       "Name: 278, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = data_no_missing[278]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     8.0\n",
       "1     6.0\n",
       "2    10.0\n",
       "3     1.0\n",
       "4     7.0\n",
       "Name: 278, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.columns = [\"Class\"]\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train_org, X_test_org, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender_Nom</th>\n",
       "      <th>Height</th>\n",
       "      <th>Weight</th>\n",
       "      <th>QRS_Dur</th>\n",
       "      <th>P-R_Int</th>\n",
       "      <th>Q-T_Int</th>\n",
       "      <th>T_Int</th>\n",
       "      <th>P_Int</th>\n",
       "      <th>QRS</th>\n",
       "      <th>...</th>\n",
       "      <th>V6270</th>\n",
       "      <th>V6271</th>\n",
       "      <th>V6272</th>\n",
       "      <th>V6273</th>\n",
       "      <th>V6274</th>\n",
       "      <th>V6275</th>\n",
       "      <th>V6276</th>\n",
       "      <th>V6277</th>\n",
       "      <th>V6278</th>\n",
       "      <th>V6279</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>144.0</td>\n",
       "      <td>357.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>-2.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>14.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>146.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>28.5</td>\n",
       "      <td>31.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>172.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>411.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>6.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>2.6</td>\n",
       "      <td>24.2</td>\n",
       "      <td>48.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>141.0</td>\n",
       "      <td>340.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>14.3</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>24.8</td>\n",
       "      <td>57.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>44.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>155.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>363.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.1</td>\n",
       "      <td>16.1</td>\n",
       "      <td>23.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 278 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Age  Gender_Nom  Height  Weight  QRS_Dur  P-R_Int  Q-T_Int  T_Int  \\\n",
       "303  67.0         0.0   176.0    80.0     97.0    144.0    357.0  170.0   \n",
       "131  48.0         1.0   165.0    70.0     83.0    146.0    418.0  134.0   \n",
       "116  74.0         0.0   172.0    74.0    106.0    165.0    411.0  161.0   \n",
       "305  14.0         0.0   175.0    59.0     96.0    141.0    340.0  225.0   \n",
       "65   44.0         1.0   155.0    65.0     80.0    117.0    363.0  142.0   \n",
       "\n",
       "     P_Int   QRS  ...    V6270  V6271  V6272  V6273  V6274  V6275  V6276  \\\n",
       "303  100.0 -48.0  ...     -0.2    0.0    3.6   -2.6    0.0    0.0    1.0   \n",
       "131   83.0  48.0  ...     -1.0    0.0   11.9    0.0    0.0    0.0    1.0   \n",
       "116   96.0  55.0  ...     -0.5   -0.5    6.5    0.0    0.0    0.0    0.3   \n",
       "305   87.0  80.0  ...      0.1   -0.8   14.3   -2.5    0.0    0.0    0.5   \n",
       "65    72.0  56.0  ...     -0.2    0.0    6.2    0.0    0.0    0.0    0.5   \n",
       "\n",
       "     V6277  V6278  V6279  \n",
       "303    1.5    0.2   14.6  \n",
       "131    0.6   28.5   31.7  \n",
       "116    2.6   24.2   48.6  \n",
       "305    3.4   24.8   57.4  \n",
       "65     1.1   16.1   23.8  \n",
       "\n",
       "[5 rows x 278 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_org.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling using MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train_org)\n",
    "X_test = scaler.transform(X_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Machine Learning Models (Without Bagging Boosting and PCA)\n",
    "\n",
    "- We will be using \"WEIGHTED RECALL\" as evaluation strategy because we are predicting Cardiac Arrhythmia, which is serious medical condition.\n",
    "- We don't want to misclassify someone having arrhythmia as normal. It is lot bigger risk than classifying someone normal as having arrhythmia. So we want to maximize true positive rate i.e. Recall.\n",
    "- Weighted recall is used instead of normal recall because it accounts for label imbalance present in data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN clasiification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 7, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid={'n_neighbors':[1,2,3,4,5,7,10]}\n",
    "\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00762138, 0.00780034, 0.00873671, 0.00838623, 0.0072885 ,\n",
       "        0.00823112, 0.00707488]),\n",
       " 'mean_score_time': array([0.11858268, 0.11493788, 0.1162931 , 0.11709194, 0.113901  ,\n",
       "        0.11549411, 0.11699977]),\n",
       " 'mean_test_score': array([0.55208333, 0.59114583, 0.59635417, 0.578125  , 0.57291667,\n",
       "        0.5625    , 0.56770833]),\n",
       " 'mean_train_score': array([1.        , 0.68618421, 0.66540019, 0.63084585, 0.62170995,\n",
       "        0.60026961, 0.57163885]),\n",
       " 'param_n_neighbors': masked_array(data=[1, 2, 3, 4, 5, 7, 10],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 1},\n",
       "  {'n_neighbors': 2},\n",
       "  {'n_neighbors': 3},\n",
       "  {'n_neighbors': 4},\n",
       "  {'n_neighbors': 5},\n",
       "  {'n_neighbors': 7},\n",
       "  {'n_neighbors': 10}],\n",
       " 'rank_test_score': array([7, 2, 1, 3, 4, 6, 5]),\n",
       " 'split0_test_score': array([0.57317073, 0.58536585, 0.59756098, 0.54878049, 0.53658537,\n",
       "        0.53658537, 0.52439024]),\n",
       " 'split0_train_score': array([1.        , 0.67218543, 0.65562914, 0.62251656, 0.62251656,\n",
       "        0.60927152, 0.5794702 ]),\n",
       " 'split1_test_score': array([0.5125, 0.5875, 0.6   , 0.575 , 0.5625, 0.5625, 0.5625]),\n",
       " 'split1_train_score': array([1.        , 0.6875    , 0.68092105, 0.62828947, 0.61184211,\n",
       "        0.58552632, 0.56578947]),\n",
       " 'split2_test_score': array([0.51948052, 0.57142857, 0.58441558, 0.54545455, 0.55844156,\n",
       "        0.53246753, 0.55844156]),\n",
       " 'split2_train_score': array([1.        , 0.70684039, 0.67100977, 0.64820847, 0.62540717,\n",
       "        0.60912052, 0.5732899 ]),\n",
       " 'split3_test_score': array([0.57534247, 0.61643836, 0.5890411 , 0.57534247, 0.57534247,\n",
       "        0.57534247, 0.5890411 ]),\n",
       " 'split3_train_score': array([1.        , 0.69131833, 0.66559486, 0.62700965, 0.62057878,\n",
       "        0.59807074, 0.56913183]),\n",
       " 'split4_test_score': array([0.58333333, 0.59722222, 0.61111111, 0.65277778, 0.63888889,\n",
       "        0.61111111, 0.61111111]),\n",
       " 'split4_train_score': array([1.        , 0.67307692, 0.65384615, 0.62820513, 0.62820513,\n",
       "        0.59935897, 0.57051282]),\n",
       " 'std_fit_time': array([0.00187214, 0.00095845, 0.00136969, 0.00285713, 0.00391158,\n",
       "        0.0020686 , 0.00152133]),\n",
       " 'std_score_time': array([0.00841164, 0.00240004, 0.00149646, 0.0023242 , 0.00196583,\n",
       "        0.0027214 , 0.00290212]),\n",
       " 'std_test_score': array([0.03033932, 0.01471036, 0.00909168, 0.03803103, 0.03411259,\n",
       "        0.02826112, 0.02938195]),\n",
       " 'std_train_score': array([0.        , 0.01282603, 0.01001371, 0.00893379, 0.00557066,\n",
       "        0.00874531, 0.00460053])}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Parameter : No. of neighbours=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5963541666666666"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.6692708333333334\n",
      "Test Recall score: 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = knn_clf.predict(X_train)\n",
    "y_pred_test = knn_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and KNN doesn't perform well.\n",
    "- But model is a good fit as train and test scores are almost same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(lreg_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.20082269, 0.14297881, 0.18642406, 0.18857894, 0.18225813,\n",
       "        0.20940337, 0.2022018 , 0.19891438]),\n",
       " 'mean_score_time': array([0.00050845, 0.00121174, 0.00120168, 0.00104451, 0.00242391,\n",
       "        0.00106215, 0.00111279, 0.00150518]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.609375  ,\n",
       "        0.70052083, 0.68489583, 0.66666667]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.64196967,\n",
       "        0.83792892, 0.96622393, 0.99481065]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 4, 1, 2, 3]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.59756098,\n",
       "        0.67073171, 0.69512195, 0.65853659]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.64238411,\n",
       "        0.8410596 , 0.97019868, 0.99668874]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.6   , 0.7125, 0.6375, 0.6375]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.64802632,\n",
       "        0.84210526, 0.97039474, 0.99342105]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.58441558,\n",
       "        0.67532468, 0.63636364, 0.62337662]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.64495114,\n",
       "        0.83713355, 0.9771987 , 1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.61643836,\n",
       "        0.69863014, 0.73972603, 0.7260274 ]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.63987138,\n",
       "        0.83601286, 0.95819936, 0.9903537 ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.65277778,\n",
       "        0.75      , 0.72222222, 0.69444444]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.63461538,\n",
       "        0.83333333, 0.95512821, 0.99358974]),\n",
       " 'std_fit_time': array([0.00371925, 0.01440043, 0.00594495, 0.02020674, 0.01520172,\n",
       "        0.00948913, 0.01311213, 0.00726705]),\n",
       " 'std_score_time': array([0.00063972, 0.00149096, 0.00146916, 0.00060285, 0.00158518,\n",
       "        0.0002447 , 0.00059149, 0.00031667]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02314586,\n",
       "        0.02839729, 0.04237194, 0.03717082]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00456579,\n",
       "        0.0032455 , 0.00826013, 0.00327849])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005208333333334"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs',C=1)\n",
    "lreg_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.8411458333333334\n",
      "Test Recall score: 0.6617647058823529\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = lreg_clf.predict(X_train)\n",
    "y_pred_test = lreg_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and logistic regression doesn't perform well.\n",
    "- Also model is overfitting as there is large difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "LSVC_clf = LinearSVC(multi_class='crammer_singer')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(LSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02250376, 0.06714101, 0.13118382, 0.26424427, 0.66457653,\n",
       "        2.30589085, 6.66223407, 6.06002626]),\n",
       " 'mean_score_time': array([0.00296249, 0.0014688 , 0.00118303, 0.0014904 , 0.00101552,\n",
       "        0.00073786, 0.00130463, 0.00296283]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.578125  , 0.69791667,\n",
       "        0.69791667, 0.609375  , 0.60677083]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.60413872, 0.78261155,\n",
       "        0.94141337, 0.99871795, 1.        ]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([6, 6, 6, 5, 1, 1, 3, 4]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.53658537, 0.68292683,\n",
       "        0.67073171, 0.62195122, 0.6097561 ]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.59933775, 0.79139073,\n",
       "        0.93708609, 1.        , 1.        ]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5625, 0.675 , 0.7   , 0.55  , 0.5625]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.60855263, 0.78618421,\n",
       "        0.94078947, 1.        , 1.        ]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.58441558, 0.7012987 ,\n",
       "        0.67532468, 0.5974026 , 0.5974026 ]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.59609121, 0.77850163,\n",
       "        0.95439739, 1.        , 1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.60273973, 0.69863014,\n",
       "        0.71232877, 0.67123288, 0.67123288]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.61414791, 0.77813505,\n",
       "        0.93569132, 1.        , 1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.61111111, 0.73611111,\n",
       "        0.73611111, 0.61111111, 0.59722222]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.6025641 , 0.77884615,\n",
       "        0.93910256, 0.99358974, 1.        ]),\n",
       " 'std_fit_time': array([0.0068105 , 0.00486168, 0.01314842, 0.04732338, 0.12888413,\n",
       "        0.25336696, 0.86890294, 0.5736565 ]),\n",
       " 'std_score_time': array([0.00107939, 0.00176574, 0.00067408, 0.00031738, 0.00065345,\n",
       "        0.00062582, 0.00068116, 0.00161457]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02732159, 0.02081613,\n",
       "        0.02395988, 0.03905063, 0.03514059]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00648009, 0.00530936,\n",
       "        0.00671984, 0.0025641 , 0.        ])}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 0.1}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6979166666666666"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC_clf = LinearSVC(multi_class='crammer_singer', C=0.1)\n",
    "LSVC_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.7838541666666666\n",
      "Test Recall score: 0.7205882352941176\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = LSVC_clf.predict(X_train)\n",
    "y_pred_test = LSVC_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is better than KNN and logistic regression, so Linear SVC performs better.\n",
    "- Also model is good fit as there is not much difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernelised SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "KSVC_clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "param_grid = {'C': [0.0001,0.001,0.01,0.1,1,10],\n",
    "          'gamma': [0.0001,0.001,0.01,0.1,1,10]}\n",
    "\n",
    "grid_search = GridSearchCV(KSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.06030979, 0.06716127, 0.07048559, 0.05828342, 0.06091447,\n",
       "        0.06661263, 0.06041207, 0.07309465, 0.07571621, 0.07810373,\n",
       "        0.07388053, 0.0863554 , 0.06987319, 0.0632894 , 0.05978227,\n",
       "        0.09247537, 0.17898526, 0.24360237, 0.0641448 , 0.06684971,\n",
       "        0.08680263, 0.10804205, 0.274931  , 0.33121161, 0.07830763,\n",
       "        0.07529593, 0.09669714, 0.09915004, 0.21826315, 0.28718529,\n",
       "        0.08964982, 0.10403247, 0.10201068, 0.12808509, 0.30487986,\n",
       "        0.35476756]),\n",
       " 'mean_score_time': array([0.01279259, 0.01532097, 0.01614127, 0.01207814, 0.01284089,\n",
       "        0.01280828, 0.01331205, 0.01491227, 0.01588969, 0.01480074,\n",
       "        0.0166368 , 0.01542611, 0.01429491, 0.01331658, 0.01229892,\n",
       "        0.01568351, 0.01738601, 0.01872816, 0.01286259, 0.01286073,\n",
       "        0.01290569, 0.01487541, 0.01890073, 0.01810036, 0.01577387,\n",
       "        0.01214061, 0.01354046, 0.01473722, 0.01436176, 0.01635094,\n",
       "        0.01562152, 0.01779146, 0.0149385 , 0.01702042, 0.02015529,\n",
       "        0.01849179]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.64583333, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.6953125 , 0.69791667, 0.54166667,\n",
       "        0.53385417]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.5371777 , 0.73766313, 0.9525008 , 1.        ,\n",
       "        0.53393884, 0.54170675, 0.76237661, 0.97595312, 1.        ,\n",
       "        1.        ]),\n",
       " 'param_C': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1,\n",
       "                    1, 1, 1, 1, 10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01,\n",
       "                    0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001,\n",
       "                    0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                    10, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.0001, 'gamma': 0.0001},\n",
       "  {'C': 0.0001, 'gamma': 0.001},\n",
       "  {'C': 0.0001, 'gamma': 0.01},\n",
       "  {'C': 0.0001, 'gamma': 0.1},\n",
       "  {'C': 0.0001, 'gamma': 1},\n",
       "  {'C': 0.0001, 'gamma': 10},\n",
       "  {'C': 0.001, 'gamma': 0.0001},\n",
       "  {'C': 0.001, 'gamma': 0.001},\n",
       "  {'C': 0.001, 'gamma': 0.01},\n",
       "  {'C': 0.001, 'gamma': 0.1},\n",
       "  {'C': 0.001, 'gamma': 1},\n",
       "  {'C': 0.001, 'gamma': 10},\n",
       "  {'C': 0.01, 'gamma': 0.0001},\n",
       "  {'C': 0.01, 'gamma': 0.001},\n",
       "  {'C': 0.01, 'gamma': 0.01},\n",
       "  {'C': 0.01, 'gamma': 0.1},\n",
       "  {'C': 0.01, 'gamma': 1},\n",
       "  {'C': 0.01, 'gamma': 10},\n",
       "  {'C': 0.1, 'gamma': 0.0001},\n",
       "  {'C': 0.1, 'gamma': 0.001},\n",
       "  {'C': 0.1, 'gamma': 0.01},\n",
       "  {'C': 0.1, 'gamma': 0.1},\n",
       "  {'C': 0.1, 'gamma': 1},\n",
       "  {'C': 0.1, 'gamma': 10},\n",
       "  {'C': 1, 'gamma': 0.0001},\n",
       "  {'C': 1, 'gamma': 0.001},\n",
       "  {'C': 1, 'gamma': 0.01},\n",
       "  {'C': 1, 'gamma': 0.1},\n",
       "  {'C': 1, 'gamma': 1},\n",
       "  {'C': 1, 'gamma': 10},\n",
       "  {'C': 10, 'gamma': 0.0001},\n",
       "  {'C': 10, 'gamma': 0.001},\n",
       "  {'C': 10, 'gamma': 0.01},\n",
       "  {'C': 10, 'gamma': 0.1},\n",
       "  {'C': 10, 'gamma': 1},\n",
       "  {'C': 10, 'gamma': 10}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 2, 1, 4, 5]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.59756098, 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.68292683, 0.68292683, 0.51219512,\n",
       "        0.5       ]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54635762, 0.73509934, 0.95033113, 1.        ,\n",
       "        0.54304636, 0.54635762, 0.76490066, 0.97682119, 1.        ,\n",
       "        1.        ]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.6375, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.675 , 0.7   , 0.525 , 0.5125]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.74342105, 0.95723684, 1.        ,\n",
       "        0.53947368, 0.54276316, 0.75986842, 0.98026316, 1.        ,\n",
       "        1.        ]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.61038961, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.66233766, 0.7012987 , 0.53246753,\n",
       "        0.53246753]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53745928, 0.74267101, 0.95765472, 1.        ,\n",
       "        0.53420195, 0.54397394, 0.76221498, 0.98045603, 1.        ,\n",
       "        1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.65753425, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.71232877, 0.68493151, 0.57534247,\n",
       "        0.56164384]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.53054662, 0.74276527, 0.95176849, 1.        ,\n",
       "        0.52733119, 0.53697749, 0.76848875, 0.97427653, 1.        ,\n",
       "        1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.73611111, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.75      , 0.72222222, 0.56944444,\n",
       "        0.56944444]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.53205128, 0.72435897, 0.94551282, 1.        ,\n",
       "        0.52564103, 0.53846154, 0.75641026, 0.96794872, 1.        ,\n",
       "        1.        ]),\n",
       " 'std_fit_time': array([0.01118696, 0.00732996, 0.00339566, 0.01199784, 0.00471667,\n",
       "        0.00603896, 0.01202963, 0.00379838, 0.00504966, 0.00684835,\n",
       "        0.00157968, 0.00530986, 0.00128317, 0.00794582, 0.0069604 ,\n",
       "        0.01443012, 0.02212   , 0.05035351, 0.00820375, 0.00750049,\n",
       "        0.00888897, 0.01597661, 0.00845698, 0.00854228, 0.00647273,\n",
       "        0.01062567, 0.01599961, 0.01200488, 0.01538621, 0.03899222,\n",
       "        0.01205274, 0.00298753, 0.00697493, 0.00523617, 0.01197168,\n",
       "        0.0159637 ]),\n",
       " 'std_score_time': array([0.00305204, 0.0024066 , 0.00112931, 0.00253571, 0.00302395,\n",
       "        0.00202857, 0.00519408, 0.00194126, 0.00283695, 0.00158989,\n",
       "        0.00139651, 0.00082049, 0.00179297, 0.00307652, 0.00227768,\n",
       "        0.00304388, 0.00245796, 0.00266238, 0.00195612, 0.00289065,\n",
       "        0.00346964, 0.00236425, 0.00270932, 0.00154136, 0.00277284,\n",
       "        0.00228858, 0.00114853, 0.003748  , 0.00290755, 0.00496151,\n",
       "        0.00095765, 0.00168508, 0.00165176, 0.00239873, 0.00177825,\n",
       "        0.00368438]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.04814706, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.03086351, 0.01392844, 0.02491146,\n",
       "        0.02691538]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00565461, 0.00731899, 0.00454023, 0.        ,\n",
       "        0.00672555, 0.00348666, 0.00413868, 0.00461492, 0.        ,\n",
       "        0.        ])}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : C=10, gamma=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6979166666666666"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "KSVC_clf = svm.SVC(kernel='rbf',C=10,gamma=0.1)\n",
    "\n",
    "KSVC_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.9765625\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = KSVC_clf.predict(X_train)\n",
    "y_pred_test = KSVC_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and Kernalised SVM doesn't perform well.\n",
    "- Also model is overfitting as there is large difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5, 6, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [2, 3, 4, 5,6, 10, 20]}\n",
    "\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01182098, 0.01528044, 0.02179918, 0.02717204, 0.02417645,\n",
       "        0.04401808, 0.06752858]),\n",
       " 'mean_score_time': array([0.00226817, 0.00100346, 0.00166602, 0.00141649, 0.00120249,\n",
       "        0.00150499, 0.00250907]),\n",
       " 'mean_test_score': array([0.6171875 , 0.65104167, 0.67447917, 0.65625   , 0.67447917,\n",
       "        0.65625   , 0.66666667]),\n",
       " 'mean_train_score': array([0.64791021, 0.71820893, 0.77155467, 0.81909989, 0.8627067 ,\n",
       "        0.96624575, 1.        ]),\n",
       " 'param_max_depth': masked_array(data=[2, 3, 4, 5, 6, 10, 20],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 10},\n",
       "  {'max_depth': 20}],\n",
       " 'rank_test_score': array([7, 6, 1, 4, 1, 4, 3]),\n",
       " 'split0_test_score': array([0.56097561, 0.6097561 , 0.63414634, 0.62195122, 0.67073171,\n",
       "        0.65853659, 0.70731707]),\n",
       " 'split0_train_score': array([0.66225166, 0.71192053, 0.7615894 , 0.81125828, 0.86092715,\n",
       "        0.97682119, 1.        ]),\n",
       " 'split1_test_score': array([0.575 , 0.6   , 0.675 , 0.6625, 0.725 , 0.675 , 0.7125]),\n",
       " 'split1_train_score': array([0.64802632, 0.74013158, 0.78618421, 0.82894737, 0.86513158,\n",
       "        0.97368421, 1.        ]),\n",
       " 'split2_test_score': array([0.5974026 , 0.64935065, 0.66233766, 0.66233766, 0.63636364,\n",
       "        0.58441558, 0.58441558]),\n",
       " 'split2_train_score': array([0.66123779, 0.73615635, 0.79478827, 0.8534202 , 0.89250814,\n",
       "        0.96742671, 1.        ]),\n",
       " 'split3_test_score': array([0.68493151, 0.69863014, 0.68493151, 0.64383562, 0.64383562,\n",
       "        0.64383562, 0.57534247]),\n",
       " 'split3_train_score': array([0.62700965, 0.68488746, 0.74598071, 0.78456592, 0.82958199,\n",
       "        0.94855305, 1.        ]),\n",
       " 'split4_test_score': array([0.68055556, 0.70833333, 0.72222222, 0.69444444, 0.69444444,\n",
       "        0.72222222, 0.75      ]),\n",
       " 'split4_train_score': array([0.64102564, 0.71794872, 0.76923077, 0.81730769, 0.86538462,\n",
       "        0.96474359, 1.        ]),\n",
       " 'std_fit_time': array([0.00127298, 0.00176438, 0.00171638, 0.00058402, 0.00403825,\n",
       "        0.00195309, 0.0152455 ]),\n",
       " 'std_score_time': array([1.09050930e-03, 6.34602175e-04, 1.94677835e-03, 2.07818490e-04,\n",
       "        2.43424907e-04, 3.91699595e-05, 1.76918459e-03]),\n",
       " 'std_test_score': array([0.05243442, 0.04419587, 0.02872541, 0.02386645, 0.03285282,\n",
       "        0.04420909, 0.07095532]),\n",
       " 'std_train_score': array([0.01317229, 0.0197618 , 0.01739286, 0.02250402, 0.02000795,\n",
       "        0.00983721, 0.        ])}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 4}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : max_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6744791666666666"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=4)\n",
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.75\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = dt_clf.predict(X_train)\n",
    "y_pred_test = dt_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is poor and decision tree doesn't perform well.\n",
    "- Also model is somewhat overfitting as there is difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=False, random_state=10, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_features': [100, 125, 150, 200], 'max_depth': [6, 8, 10, 12, 14], 'max_leaf_nodes': [20, 22, 30, 50]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',n_jobs= -1,random_state=10)\n",
    "\n",
    "param_grid = {'max_features': [100,125,150,200],\n",
    "          'max_depth': [6,8,10,12,14],\n",
    "           'max_leaf_nodes':[20,22,30,50]}\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([1.05817866, 1.13703184, 0.96248488, 1.03218384, 1.34079223,\n",
       "        1.37787457, 1.24069099, 1.21030602, 1.33798671, 1.3563683 ,\n",
       "        1.45740261, 1.56993146, 1.57798958, 1.62058187, 1.8045126 ,\n",
       "        1.63484211, 1.30532136, 1.31167459, 1.31384811, 1.24995799,\n",
       "        1.49451947, 1.45090532, 1.52943316, 1.406915  , 1.61174874,\n",
       "        1.64421811, 1.66761022, 1.59551539, 1.92139764, 1.99900608,\n",
       "        1.97438889, 1.98496685, 1.39027963, 1.32272472, 1.16238484,\n",
       "        1.16500826, 1.38323421, 1.32395477, 1.28452144, 1.25006595,\n",
       "        1.34128418, 1.42402616, 1.47317944, 1.4259654 , 1.73890505,\n",
       "        1.67893476, 1.84885297, 1.66389675, 1.10986547, 1.19443517,\n",
       "        1.16203184, 1.25804796, 1.33466763, 1.28418012, 1.32359052,\n",
       "        1.44121981, 1.28083091, 1.30033045, 1.52439909, 1.32607722,\n",
       "        1.47940273, 1.61456137, 1.91717005, 1.64939094, 1.06808796,\n",
       "        1.11333671, 1.19456391, 1.32355509, 1.16075969, 1.30092664,\n",
       "        1.33680553, 1.46544747, 1.39005394, 1.35715628, 1.49923282,\n",
       "        1.47149844, 1.60452476, 1.58743119, 1.8043807 , 1.81570382]),\n",
       " 'mean_score_time': array([0.19265628, 0.29809513, 0.15111718, 0.12705712, 0.127246  ,\n",
       "        0.10773129, 0.10789495, 0.19331932, 0.10758634, 0.21340632,\n",
       "        0.23293548, 0.15055027, 0.15106654, 0.12833862, 0.15243211,\n",
       "        0.14954629, 0.15086951, 0.19315658, 0.1927424 , 0.10842204,\n",
       "        0.17086124, 0.23581934, 0.13002973, 0.12917056, 0.17249846,\n",
       "        0.13087559, 0.2146966 , 0.17250562, 0.15266132, 0.1712976 ,\n",
       "        0.15127921, 0.19214258, 0.1083818 , 0.2340909 , 0.12705874,\n",
       "        0.12997265, 0.12998438, 0.1915586 , 0.21147175, 0.17088094,\n",
       "        0.12974067, 0.17263665, 0.15110302, 0.21540484, 0.17186532,\n",
       "        0.19313374, 0.19305253, 0.19094892, 0.21341982, 0.21167488,\n",
       "        0.14923139, 0.14858871, 0.21434121, 0.25869617, 0.19428673,\n",
       "        0.15266142, 0.23159814, 0.1272274 , 0.14756112, 0.10543375,\n",
       "        0.16823344, 0.12768216, 0.19366527, 0.19087672, 0.16976562,\n",
       "        0.27739944, 0.17242751, 0.14928336, 0.10784397, 0.19213634,\n",
       "        0.1919374 , 0.12884855, 0.19284697, 0.2537343 , 0.1482152 ,\n",
       "        0.15095983, 0.10902653, 0.10776033, 0.1916708 , 0.14987226]),\n",
       " 'mean_test_score': array([0.73177083, 0.734375  , 0.73177083, 0.73177083, 0.7265625 ,\n",
       "        0.7265625 , 0.72916667, 0.72916667, 0.74479167, 0.74479167,\n",
       "        0.74479167, 0.74479167, 0.734375  , 0.7421875 , 0.734375  ,\n",
       "        0.734375  , 0.75      , 0.75260417, 0.75      , 0.75      ,\n",
       "        0.7421875 , 0.7421875 , 0.74739583, 0.75      , 0.75      ,\n",
       "        0.75      , 0.75260417, 0.75260417, 0.74739583, 0.75      ,\n",
       "        0.75260417, 0.75      , 0.75      , 0.75260417, 0.7578125 ,\n",
       "        0.75520833, 0.7421875 , 0.74479167, 0.74479167, 0.74739583,\n",
       "        0.75      , 0.75      , 0.75260417, 0.75260417, 0.74739583,\n",
       "        0.75      , 0.75      , 0.75260417, 0.75      , 0.75260417,\n",
       "        0.75520833, 0.75520833, 0.7421875 , 0.74739583, 0.74739583,\n",
       "        0.74479167, 0.75      , 0.75      , 0.75      , 0.75260417,\n",
       "        0.74739583, 0.75      , 0.75      , 0.75      , 0.75      ,\n",
       "        0.75260417, 0.75520833, 0.75260417, 0.7421875 , 0.74739583,\n",
       "        0.74739583, 0.74479167, 0.75      , 0.75      , 0.75      ,\n",
       "        0.75260417, 0.74739583, 0.75      , 0.75      , 0.74739583]),\n",
       " 'mean_train_score': array([0.89262431, 0.90885622, 0.9225074 , 0.92507562, 0.89910615,\n",
       "        0.91475282, 0.93029482, 0.93288221, 0.90175343, 0.91604118,\n",
       "        0.92900863, 0.93029481, 0.90566235, 0.91869982, 0.93163792,\n",
       "        0.93358634, 0.89907884, 0.91142905, 0.95962135, 0.97330675,\n",
       "        0.90102085, 0.915381  , 0.96091213, 0.97133514, 0.90630958,\n",
       "        0.91934523, 0.96287536, 0.97323675, 0.9076125 , 0.92515196,\n",
       "        0.96811108, 0.97783786, 0.89912968, 0.91604738, 0.97271416,\n",
       "        0.99153174, 0.90367159, 0.91737154, 0.98114672, 0.98958778,\n",
       "        0.90629271, 0.92258166, 0.97983943, 0.99153174, 0.9075977 ,\n",
       "        0.92844578, 0.98443446, 0.99218963, 0.89980881, 0.91735032,\n",
       "        0.97596551, 0.99350978, 0.90365679, 0.91801865, 0.98312706,\n",
       "        0.99350978, 0.90695061, 0.92389745, 0.98506264, 0.9941508 ,\n",
       "        0.9082556 , 0.92975915, 0.98766896, 0.9941508 , 0.89980881,\n",
       "        0.91735032, 0.97985389, 0.99547095, 0.90365679, 0.91801865,\n",
       "        0.98442161, 0.99547095, 0.90695061, 0.92323956, 0.98442161,\n",
       "        0.99547095, 0.9082556 , 0.92975915, 0.98831204, 0.99675506]),\n",
       " 'param_max_depth': masked_array(data=[6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 8, 8,\n",
       "                    8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 10, 10, 10,\n",
       "                    10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 12,\n",
       "                    12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12,\n",
       "                    12, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14,\n",
       "                    14, 14, 14],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_features': masked_array(data=[100, 100, 100, 100, 125, 125, 125, 125, 150, 150, 150,\n",
       "                    150, 200, 200, 200, 200, 100, 100, 100, 100, 125, 125,\n",
       "                    125, 125, 150, 150, 150, 150, 200, 200, 200, 200, 100,\n",
       "                    100, 100, 100, 125, 125, 125, 125, 150, 150, 150, 150,\n",
       "                    200, 200, 200, 200, 100, 100, 100, 100, 125, 125, 125,\n",
       "                    125, 150, 150, 150, 150, 200, 200, 200, 200, 100, 100,\n",
       "                    100, 100, 125, 125, 125, 125, 150, 150, 150, 150, 200,\n",
       "                    200, 200, 200],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_leaf_nodes': masked_array(data=[20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50,\n",
       "                    20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50,\n",
       "                    20, 22, 30, 50, 20, 22, 30, 50, 20, 22, 30, 50, 20, 22,\n",
       "                    30, 50, 20, 22, 30, 50, 20, 22, 30, 50],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 6, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 8, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 10, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 12, 'max_features': 200, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 100, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 125, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 150, 'max_leaf_nodes': 50},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 20},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 22},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 30},\n",
       "  {'max_depth': 14, 'max_features': 200, 'max_leaf_nodes': 50}],\n",
       " 'rank_test_score': array([74, 70, 74, 74, 79, 79, 77, 77, 56, 56, 56, 56, 70, 64, 70, 70, 19,\n",
       "         6, 19, 19, 64, 64, 45, 19, 19, 19,  6,  6, 45, 19,  6, 19, 19,  6,\n",
       "         1,  2, 64, 56, 56, 45, 19, 19,  6,  6, 45, 19, 19,  6, 19,  6,  2,\n",
       "         2, 64, 45, 45, 56, 19, 19, 19,  6, 45, 19, 19, 19, 19,  6,  2,  6,\n",
       "        64, 45, 45, 56, 19, 19, 19,  6, 45, 19, 19, 45]),\n",
       " 'split0_test_score': array([0.68292683, 0.68292683, 0.68292683, 0.68292683, 0.67073171,\n",
       "        0.67073171, 0.67073171, 0.67073171, 0.7195122 , 0.7195122 ,\n",
       "        0.7195122 , 0.7195122 , 0.69512195, 0.70731707, 0.69512195,\n",
       "        0.69512195, 0.7195122 , 0.7195122 , 0.70731707, 0.70731707,\n",
       "        0.69512195, 0.69512195, 0.70731707, 0.70731707, 0.73170732,\n",
       "        0.73170732, 0.73170732, 0.73170732, 0.7195122 , 0.73170732,\n",
       "        0.73170732, 0.73170732, 0.7195122 , 0.7195122 , 0.73170732,\n",
       "        0.73170732, 0.69512195, 0.70731707, 0.70731707, 0.7195122 ,\n",
       "        0.73170732, 0.73170732, 0.73170732, 0.73170732, 0.7195122 ,\n",
       "        0.73170732, 0.73170732, 0.73170732, 0.7195122 , 0.7195122 ,\n",
       "        0.73170732, 0.73170732, 0.69512195, 0.70731707, 0.7195122 ,\n",
       "        0.7195122 , 0.73170732, 0.73170732, 0.73170732, 0.73170732,\n",
       "        0.7195122 , 0.73170732, 0.73170732, 0.73170732, 0.7195122 ,\n",
       "        0.7195122 , 0.73170732, 0.73170732, 0.69512195, 0.70731707,\n",
       "        0.7195122 , 0.7195122 , 0.73170732, 0.73170732, 0.73170732,\n",
       "        0.73170732, 0.7195122 , 0.73170732, 0.73170732, 0.73170732]),\n",
       " 'split0_train_score': array([0.89735099, 0.90728477, 0.9205298 , 0.9205298 , 0.90066225,\n",
       "        0.92384106, 0.93046358, 0.93377483, 0.9205298 , 0.92715232,\n",
       "        0.93377483, 0.93377483, 0.91721854, 0.92715232, 0.93046358,\n",
       "        0.93377483, 0.90066225, 0.91059603, 0.95695364, 0.97350993,\n",
       "        0.90066225, 0.9205298 , 0.9602649 , 0.97019868, 0.91059603,\n",
       "        0.92384106, 0.96357616, 0.96357616, 0.91390728, 0.92384106,\n",
       "        0.97019868, 0.97350993, 0.90397351, 0.9205298 , 0.97350993,\n",
       "        0.99337748, 0.90397351, 0.92384106, 0.98344371, 0.99337748,\n",
       "        0.91059603, 0.92715232, 0.98013245, 0.99337748, 0.91059603,\n",
       "        0.93046358, 0.99337748, 0.99337748, 0.90728477, 0.9205298 ,\n",
       "        0.97682119, 0.99668874, 0.90397351, 0.9205298 , 0.99006623,\n",
       "        0.99668874, 0.91059603, 0.92715232, 0.99006623, 0.99668874,\n",
       "        0.91059603, 0.93046358, 0.99337748, 0.99668874, 0.90728477,\n",
       "        0.9205298 , 0.98013245, 1.        , 0.90397351, 0.9205298 ,\n",
       "        0.99006623, 1.        , 0.91059603, 0.92715232, 0.99006623,\n",
       "        1.        , 0.91059603, 0.93046358, 0.99337748, 1.        ]),\n",
       " 'split1_test_score': array([0.7   , 0.7   , 0.7   , 0.7   , 0.7   , 0.7   , 0.7   , 0.7   ,\n",
       "        0.7125, 0.7125, 0.7125, 0.7125, 0.7   , 0.7125, 0.6875, 0.6875,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125, 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125, 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125,\n",
       "        0.725 , 0.725 , 0.725 , 0.7125, 0.7125, 0.725 , 0.725 , 0.725 ,\n",
       "        0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.725 , 0.7125, 0.7125]),\n",
       " 'split1_train_score': array([0.88815789, 0.90460526, 0.91447368, 0.91447368, 0.89473684,\n",
       "        0.91118421, 0.91776316, 0.91776316, 0.88815789, 0.90460526,\n",
       "        0.91118421, 0.91118421, 0.89802632, 0.91776316, 0.92763158,\n",
       "        0.92763158, 0.89144737, 0.90460526, 0.95394737, 0.97039474,\n",
       "        0.89473684, 0.91118421, 0.95394737, 0.96710526, 0.91118421,\n",
       "        0.92763158, 0.95723684, 0.97039474, 0.90460526, 0.92763158,\n",
       "        0.96381579, 0.97368421, 0.89802632, 0.91776316, 0.97697368,\n",
       "        0.98684211, 0.90789474, 0.91776316, 0.98026316, 0.98684211,\n",
       "        0.90789474, 0.92105263, 0.98355263, 0.98684211, 0.90789474,\n",
       "        0.93421053, 0.98355263, 0.99013158, 0.90131579, 0.91776316,\n",
       "        0.97697368, 0.99342105, 0.90460526, 0.92105263, 0.98355263,\n",
       "        0.99342105, 0.91118421, 0.92763158, 0.98355263, 0.99342105,\n",
       "        0.91118421, 0.93421053, 0.98684211, 0.99342105, 0.90131579,\n",
       "        0.91776316, 0.98026316, 0.99671053, 0.90460526, 0.92105263,\n",
       "        0.98355263, 0.99671053, 0.91118421, 0.92434211, 0.98355263,\n",
       "        0.99671053, 0.91118421, 0.93421053, 0.98684211, 0.99671053]),\n",
       " 'split2_test_score': array([0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987, 0.7012987,\n",
       "        0.7012987, 0.7012987]),\n",
       " 'split2_train_score': array([0.90553746, 0.91856678, 0.93159609, 0.93159609, 0.90879479,\n",
       "        0.91530945, 0.93485342, 0.93485342, 0.90553746, 0.9218241 ,\n",
       "        0.93811075, 0.93811075, 0.91530945, 0.92833876, 0.93811075,\n",
       "        0.93811075, 0.90553746, 0.91530945, 0.96742671, 0.9771987 ,\n",
       "        0.90553746, 0.91856678, 0.96416938, 0.97394137, 0.90879479,\n",
       "        0.9218241 , 0.96416938, 0.97394137, 0.91856678, 0.93159609,\n",
       "        0.97394137, 0.98371336, 0.90553746, 0.91530945, 0.98371336,\n",
       "        0.99348534, 0.90553746, 0.9218241 , 0.98697068, 0.98697068,\n",
       "        0.90879479, 0.93485342, 0.98045603, 0.99348534, 0.91530945,\n",
       "        0.93485342, 0.98697068, 0.99348534, 0.90553746, 0.9218241 ,\n",
       "        0.99022801, 0.99348534, 0.90553746, 0.92508143, 0.98697068,\n",
       "        0.99348534, 0.90879479, 0.93485342, 0.99022801, 0.99348534,\n",
       "        0.91530945, 0.94462541, 0.99022801, 0.99348534, 0.90553746,\n",
       "        0.9218241 , 0.99022801, 0.99348534, 0.90553746, 0.92508143,\n",
       "        0.99022801, 0.99348534, 0.90879479, 0.93485342, 0.99022801,\n",
       "        0.99348534, 0.91530945, 0.94462541, 0.99022801, 0.99348534]),\n",
       " 'split3_test_score': array([0.78082192, 0.78082192, 0.78082192, 0.78082192, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.80821918, 0.80821918, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.80821918, 0.80821918, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.80821918, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.80821918, 0.79452055, 0.78082192,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.80821918, 0.80821918,\n",
       "        0.80821918, 0.79452055, 0.80821918, 0.80821918, 0.79452055,\n",
       "        0.78082192, 0.79452055, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.80821918,\n",
       "        0.80821918, 0.80821918, 0.79452055, 0.80821918, 0.80821918,\n",
       "        0.79452055, 0.78082192, 0.79452055, 0.79452055, 0.79452055,\n",
       "        0.79452055, 0.79452055, 0.79452055, 0.79452055, 0.79452055]),\n",
       " 'split3_train_score': array([0.88745981, 0.91318328, 0.93247588, 0.93890675, 0.89710611,\n",
       "        0.90996785, 0.93890675, 0.94212219, 0.89710611, 0.90675241,\n",
       "        0.93569132, 0.94212219, 0.90032154, 0.91639871, 0.93890675,\n",
       "        0.94533762, 0.89710611, 0.90996785, 0.97106109, 0.97427653,\n",
       "        0.90032154, 0.90675241, 0.96784566, 0.97427653, 0.90353698,\n",
       "        0.90996785, 0.97106109, 0.9807074 , 0.90675241, 0.91961415,\n",
       "        0.97106109, 0.98392283, 0.89067524, 0.90996785, 0.96463023,\n",
       "        0.99356913, 0.89710611, 0.90996785, 0.97749196, 0.99356913,\n",
       "        0.90353698, 0.91318328, 0.97749196, 0.99356913, 0.90675241,\n",
       "        0.92282958, 0.9807074 , 0.99356913, 0.89067524, 0.90996785,\n",
       "        0.97106109, 0.99356913, 0.90032154, 0.90996785, 0.97427653,\n",
       "        0.99356913, 0.90353698, 0.91318328, 0.97749196, 0.99356913,\n",
       "        0.90675241, 0.92282958, 0.98392283, 0.99356913, 0.89067524,\n",
       "        0.90996785, 0.97749196, 0.99356913, 0.90032154, 0.90996785,\n",
       "        0.97749196, 0.99356913, 0.90353698, 0.91318328, 0.97749196,\n",
       "        0.99356913, 0.90675241, 0.92282958, 0.98713826, 0.99678457]),\n",
       " 'split4_test_score': array([0.80555556, 0.81944444, 0.80555556, 0.80555556, 0.77777778,\n",
       "        0.77777778, 0.79166667, 0.79166667, 0.79166667, 0.79166667,\n",
       "        0.79166667, 0.79166667, 0.79166667, 0.80555556, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.81944444, 0.81944444,\n",
       "        0.80555556, 0.80555556, 0.80555556, 0.81944444, 0.80555556,\n",
       "        0.80555556, 0.81944444, 0.81944444, 0.80555556, 0.80555556,\n",
       "        0.81944444, 0.80555556, 0.80555556, 0.81944444, 0.83333333,\n",
       "        0.81944444, 0.80555556, 0.80555556, 0.80555556, 0.81944444,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.81944444, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.80555556, 0.81944444,\n",
       "        0.81944444, 0.83333333, 0.80555556, 0.80555556, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.80555556, 0.80555556, 0.81944444,\n",
       "        0.80555556, 0.80555556, 0.81944444, 0.81944444, 0.80555556,\n",
       "        0.81944444, 0.81944444, 0.83333333, 0.80555556, 0.80555556,\n",
       "        0.80555556, 0.80555556, 0.80555556, 0.80555556, 0.80555556,\n",
       "        0.81944444, 0.80555556, 0.80555556, 0.81944444, 0.80555556]),\n",
       " 'split4_train_score': array([0.88461538, 0.90064103, 0.91346154, 0.91987179, 0.89423077,\n",
       "        0.91346154, 0.92948718, 0.93589744, 0.8974359 , 0.91987179,\n",
       "        0.92628205, 0.92628205, 0.8974359 , 0.90384615, 0.92307692,\n",
       "        0.92307692, 0.90064103, 0.91666667, 0.94871795, 0.97115385,\n",
       "        0.90384615, 0.91987179, 0.95833333, 0.97115385, 0.8974359 ,\n",
       "        0.91346154, 0.95833333, 0.9775641 , 0.89423077, 0.92307692,\n",
       "        0.96153846, 0.97435897, 0.8974359 , 0.91666667, 0.96474359,\n",
       "        0.99038462, 0.90384615, 0.91346154, 0.9775641 , 0.98717949,\n",
       "        0.90064103, 0.91666667, 0.9775641 , 0.99038462, 0.8974359 ,\n",
       "        0.91987179, 0.9775641 , 0.99038462, 0.89423077, 0.91666667,\n",
       "        0.96474359, 0.99038462, 0.90384615, 0.91346154, 0.98076923,\n",
       "        0.99038462, 0.90064103, 0.91666667, 0.98397436, 0.99358974,\n",
       "        0.8974359 , 0.91666667, 0.98397436, 0.99358974, 0.89423077,\n",
       "        0.91666667, 0.97115385, 0.99358974, 0.90384615, 0.91346154,\n",
       "        0.98076923, 0.99358974, 0.90064103, 0.91666667, 0.98076923,\n",
       "        0.99358974, 0.8974359 , 0.91666667, 0.98397436, 0.99679487]),\n",
       " 'std_fit_time': array([0.02765082, 0.13006603, 0.1003259 , 0.18657079, 0.17480163,\n",
       "        0.13035606, 0.15974161, 0.04049947, 0.0507163 , 0.13606266,\n",
       "        0.14801406, 0.16343572, 0.05466481, 0.09735864, 0.10243623,\n",
       "        0.04637738, 0.09256856, 0.07439475, 0.06598417, 0.0788833 ,\n",
       "        0.11393218, 0.09022729, 0.17563323, 0.13876935, 0.19567013,\n",
       "        0.09379553, 0.11873977, 0.18110875, 0.1535456 , 0.13198633,\n",
       "        0.21470173, 0.17025986, 0.13285544, 0.10984869, 0.12312555,\n",
       "        0.09193702, 0.15175122, 0.10127695, 0.04641381, 0.06646297,\n",
       "        0.11136279, 0.12638863, 0.08627131, 0.07668344, 0.08862368,\n",
       "        0.1329059 , 0.06502169, 0.08591835, 0.0510074 , 0.08337619,\n",
       "        0.05027351, 0.0596861 , 0.05934818, 0.11546747, 0.10208145,\n",
       "        0.21422053, 0.09067277, 0.15449561, 0.1394856 , 0.03444117,\n",
       "        0.06767985, 0.15956804, 0.07662616, 0.05408838, 0.13082286,\n",
       "        0.03775148, 0.06614045, 0.17716462, 0.07362348, 0.2364447 ,\n",
       "        0.07876502, 0.1873978 , 0.11087032, 0.02371816, 0.14713787,\n",
       "        0.12115267, 0.11188796, 0.14461291, 0.23951484, 0.06388944]),\n",
       " 'std_score_time': array([0.10219379, 0.04308393, 0.08860547, 0.04103366, 0.04041414,\n",
       "        0.00125395, 0.00115274, 0.1038265 , 0.00155965, 0.0956019 ,\n",
       "        0.10221953, 0.08540188, 0.08561588, 0.04171799, 0.08472002,\n",
       "        0.08487555, 0.08245319, 0.10466908, 0.10331384, 0.00185111,\n",
       "        0.08414915, 0.10274954, 0.04459656, 0.04358158, 0.08227608,\n",
       "        0.04323151, 0.09326596, 0.08618002, 0.0864563 , 0.08544244,\n",
       "        0.08372226, 0.10298948, 0.00123842, 0.10355614, 0.04196747,\n",
       "        0.04200318, 0.04149312, 0.10462259, 0.06817446, 0.08515679,\n",
       "        0.04280014, 0.08557051, 0.08505627, 0.09673467, 0.0835377 ,\n",
       "        0.1055893 , 0.10486404, 0.10422001, 0.09273624, 0.09433058,\n",
       "        0.04960605, 0.04981286, 0.09526867, 0.08450043, 0.10621166,\n",
       "        0.0830507 , 0.10401817, 0.04376851, 0.08370315, 0.00096258,\n",
       "        0.08353377, 0.04379361, 0.07946853, 0.10367006, 0.08636933,\n",
       "        0.08313839, 0.08581969, 0.08340795, 0.00161612, 0.10249584,\n",
       "        0.07970673, 0.04181418, 0.08023411, 0.08537342, 0.08442447,\n",
       "        0.08664086, 0.0022743 , 0.00147239, 0.10265179, 0.08569543]),\n",
       " 'std_test_score': array([0.04882739, 0.0528942 , 0.04882739, 0.04882739, 0.04806635,\n",
       "        0.04806635, 0.05105397, 0.05105397, 0.04370129, 0.04370129,\n",
       "        0.04370129, 0.04370129, 0.04580083, 0.04529983, 0.05141855,\n",
       "        0.05141855, 0.04502612, 0.04843646, 0.05043235, 0.05043235,\n",
       "        0.05073156, 0.05073156, 0.04702254, 0.05043235, 0.04040752,\n",
       "        0.04040752, 0.04417587, 0.04417587, 0.04186907, 0.04040752,\n",
       "        0.04417587, 0.04040752, 0.04502612, 0.04843646, 0.05062276,\n",
       "        0.04689044, 0.05073156, 0.04851324, 0.04385456, 0.04338589,\n",
       "        0.04040752, 0.04040752, 0.04417587, 0.04417587, 0.04186907,\n",
       "        0.04040752, 0.04040752, 0.04417587, 0.04502612, 0.04843646,\n",
       "        0.04689044, 0.04825993, 0.05073156, 0.04702254, 0.04186907,\n",
       "        0.03919782, 0.04040752, 0.04040752, 0.04040752, 0.04417587,\n",
       "        0.04186907, 0.04040752, 0.04605485, 0.04605485, 0.04502612,\n",
       "        0.04843646, 0.04689044, 0.05012112, 0.05073156, 0.04702254,\n",
       "        0.04186907, 0.03919782, 0.04040752, 0.04040752, 0.04040752,\n",
       "        0.04417587, 0.04186907, 0.04040752, 0.04605485, 0.04229358]),\n",
       " 'std_train_score': array([0.00774409, 0.0063411 , 0.00815174, 0.00887651, 0.00535073,\n",
       "        0.00490315, 0.00710872, 0.00809613, 0.0108807 , 0.00881629,\n",
       "        0.00975057, 0.01090324, 0.00873063, 0.00884486, 0.00609022,\n",
       "        0.00780391, 0.0046646 , 0.0042868 , 0.00836644, 0.00241842,\n",
       "        0.00370104, 0.00545696, 0.00478039, 0.00263289, 0.00519117,\n",
       "        0.00659658, 0.00493142, 0.00594032, 0.00834835, 0.00410772,\n",
       "        0.00466265, 0.00489152, 0.00529313, 0.0034907 , 0.00732993,\n",
       "        0.00263374, 0.00359287, 0.00513685, 0.0036378 , 0.00317493,\n",
       "        0.00365735, 0.00770898, 0.00223346, 0.00263374, 0.00587411,\n",
       "        0.00605667, 0.00544574, 0.00158029, 0.00641116, 0.00412947,\n",
       "        0.00842796, 0.00199421, 0.00177181, 0.00549503, 0.00542214,\n",
       "        0.00199421, 0.00414915, 0.00789492, 0.00474334, 0.0012704 ,\n",
       "        0.00605205, 0.00959946, 0.00367432, 0.0012704 , 0.00641116,\n",
       "        0.00412947, 0.0061504 , 0.00257478, 0.00177181, 0.00549503,\n",
       "        0.00505353, 0.00257478, 0.00414915, 0.00769076, 0.00505353,\n",
       "        0.00257478, 0.00605205, 0.00959946, 0.00321508, 0.00206035])}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 10, 'max_features': 100, 'max_leaf_nodes': 30}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : max_features=100, max_depth=10, max_leaf_nodes=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7578125"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=10, max_features=100, max_leaf_nodes=30,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',max_features=100,max_depth=10,max_leaf_nodes=30)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.9557291666666666\n",
      "Test Recall score: 0.7647058823529411\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = rf_clf.predict(X_train)\n",
    "y_pred_test = rf_clf.predict(X_test)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that test score is good and Random forest performs well as it is ensemble method.\n",
    "- But model is overfitting as there is large difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.85\n",
      "Test score: 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "hard_voting_clf = VotingClassifier(estimators=[('knn', knn_clf),('lr',lreg_clf),('lsvc', LSVC_clf),\n",
    "                                   ('ksvc', KSVC_clf),('dt', dt_clf), ('rt', rf_clf)],voting = 'hard')\n",
    "hard_voting_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(hard_voting_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(hard_voting_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(estimator=hard_voting_clf,X=X_train,y=y_train, scoring='recall_weighted', cv=kFold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Score: 0.71\n",
      "Mean Std: 0.02\n"
     ]
    }
   ],
   "source": [
    "print('Mean Score: {0:0.2f}'.format(score.mean()))\n",
    "print('Mean Std: {0:0.2f}'.format(score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Machine Learning Models with Bagging and/or Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "bag_knn = BaggingClassifier(base_estimator=knn_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_knn, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5448364251823543\n"
     ]
    }
   ],
   "source": [
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, Bagging is giving us a very low score. It doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pasting with KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "bag_knn = BaggingClassifier(base_estimator=knn_clf, n_estimators=100,bootstrap_features=True, bootstrap=False,\n",
    "                            max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_knn, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5448364251823543\n"
     ]
    }
   ],
   "source": [
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pasting is also giving us a very low score. It doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Logistic Reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5493598087755054\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "bag_log = BaggingClassifier(base_estimator=lreg_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_log, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm decreased the variance, but raised the bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.6398795000639281\n"
     ]
    }
   ],
   "source": [
    "bag_lsvc = BaggingClassifier(base_estimator=LSVC_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_lsvc, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.5556073763471118\n"
     ]
    }
   ],
   "source": [
    "bag_ksvc = BaggingClassifier(base_estimator=KSVC_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_ksvc, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean score: 0.6156329272465049\n"
     ]
    }
   ],
   "source": [
    "bag_dt = BaggingClassifier(base_estimator=dt_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_dt, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-bb6b2f6e17c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mbag_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbootstrap_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbag_rf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_scaled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'recall_weighted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkFold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Mean score:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[0;32m    340\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                                 pre_dispatch=pre_dispatch)\n\u001b[0m\u001b[0;32m    343\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m             return_times=True)\n\u001b[1;32m--> 206\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 638\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    639\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    640\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bag_rf = BaggingClassifier(base_estimator=rf_clf, n_estimators=100,bootstrap_features=True, max_samples=50, max_features=100)\n",
    "\n",
    "score = cross_val_score(estimator=bag_rf, X=X_scaled, y=y, scoring='recall_weighted', cv=kFold, n_jobs=-1)\n",
    "print('Mean score:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bagging algorithm has raised the bias and doesn't improve our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive boosting with decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.99\n",
      "Test score: 0.65\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = dt_clf, learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact raise the average training accuracy \n",
    "for the Decision Tree but the test accuracy got reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdBoosting with Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-1c6bc052f168>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0madaboost_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0madaboost_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train score: {0:0.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madaboost_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score: {0:0.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madaboost_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m         \u001b[1;31m# Fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    143\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 random_state)\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m# Early termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    471\u001b[0m         \"\"\"\n\u001b[0;32m    472\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'SAMME.R'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[1;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[0;32m    481\u001b[0m         \u001b[0mestimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 483\u001b[1;33m         \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[0my_predict_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 328\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    360\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = rf_clf, learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact raise the average training accuracy for the Decision Tree but the test accuracy got reduced. It is still overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.57\n",
      "Test score: 0.60\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = lreg_clf, learning_rate = 0.5)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting improved our test accuracy with logistic model and has an improved model fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.78\n",
      "Test score: 0.72\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = LSVC_clf, algorithm='SAMME')\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting did in fact reduced the average test accuracy for the Decision Tree but the test accuracy got increased. Therefore, it has a poor fit with Linear SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with KNN\n",
    "- KNeighborsClassifier does not support sample weights, so we will not be able to use Adaptive Boosting to lower the model bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Boosting with Kernel SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-12bd16e76e5d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0madaboost_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAdaBoostClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKSVC_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgorithm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'SAMME'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0madaboost_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train score: {0:0.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madaboost_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Test score: {0:0.2f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madaboost_clf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mscore\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    347\u001b[0m         \"\"\"\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 349\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    600\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m         \"\"\"\n\u001b[1;32m--> 602\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    604\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_classes_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    672\u001b[0m             pred = sum((estimator.predict(X) == classes).T * w\n\u001b[0;32m    673\u001b[0m                        for estimator, w in zip(self.estimators_,\n\u001b[1;32m--> 674\u001b[1;33m                                                self.estimator_weights_))\n\u001b[0m\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator_weights_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\ensemble\\weight_boosting.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# self.algorithm == \"SAMME\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m             pred = sum((estimator.predict(X) == classes).T * w\n\u001b[1;32m--> 673\u001b[1;33m                        for estimator, w in zip(self.estimators_,\n\u001b[0m\u001b[0;32m    674\u001b[0m                                                self.estimator_weights_))\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    546\u001b[0m             \u001b[0mClass\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \"\"\"\n\u001b[1;32m--> 548\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mBaseSVC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    549\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 310\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\sklearn\\svm\\base.py\u001b[0m in \u001b[0;36m_dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    331\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobA_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobB_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msvm_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msvm_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m             \u001b[0mdegree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoef0\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 333\u001b[1;33m             cache_size=self.cache_size)\n\u001b[0m\u001b[0;32m    334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_sparse_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "adaboost_clf = AdaBoostClassifier(base_estimator = KSVC_clf, algorithm='SAMME')\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(adaboost_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(adaboost_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that adaptive boosting has very low accuracy for Kernel SVC. So it's not a good fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gb_clf = GradientBoostingClassifier(learning_rate = 0.05)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(gb_clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is overfitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB = GradientBoostingClassifier()\n",
    "score = cross_val_score(estimator=GB, X=X_scaled, y=y, cv=kFold, n_jobs=-1)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(gb_clf.score(X_test, y_test)))\n",
    "print('Mean Accuracy:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting is overfitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB2 = GradientBoostingClassifier(min_samples_leaf=9, learning_rate=0.05, n_estimators=100)\n",
    "score = cross_val_score(estimator=GB, X=X_scaled, y=y, cv=kFold, n_jobs=-1)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "print('Train score: {0:0.2f}'.format(gb_clf.score(X_train, y_train)))\n",
    "print('Test score: {0:0.2f}'.format(gb_clf.score(X_test, y_test)))\n",
    "print('Mean Accuracy:', score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is clear that, while the bagging and boosting techniques mentioned above are \n",
    "#### usually effective, most did not do much to improve the models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Due to reasons like imbalanced classes, high dimensionality and lack of observations, we couldn't get an optimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=100, svd_solver='auto')\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "X_comb_pca = np.concatenate((X_train_pca, X_test_pca), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384, 100)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'> Machine Learning Models with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Classification with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': [1, 2, 3, 4, 5, 7, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_clf = KNeighborsClassifier(n_jobs=-1)\n",
    "\n",
    "param_grid={'n_neighbors':[1,2,3,4,5,7,10]}\n",
    "\n",
    "grid_search = GridSearchCV(knn_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.00339355, 0.00373526, 0.00351295, 0.00523491, 0.00412474,\n",
       "        0.00372815, 0.00504613]),\n",
       " 'mean_score_time': array([0.11380005, 0.11183434, 0.11402578, 0.1134593 , 0.11600695,\n",
       "        0.11262069, 0.11519852]),\n",
       " 'mean_test_score': array([0.5546875 , 0.58854167, 0.6015625 , 0.58854167, 0.58333333,\n",
       "        0.56770833, 0.57291667]),\n",
       " 'mean_train_score': array([1.        , 0.69723114, 0.67579153, 0.63796436, 0.62625759,\n",
       "        0.60225821, 0.57490731]),\n",
       " 'param_n_neighbors': masked_array(data=[1, 2, 3, 4, 5, 7, 10],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'n_neighbors': 1},\n",
       "  {'n_neighbors': 2},\n",
       "  {'n_neighbors': 3},\n",
       "  {'n_neighbors': 4},\n",
       "  {'n_neighbors': 5},\n",
       "  {'n_neighbors': 7},\n",
       "  {'n_neighbors': 10}],\n",
       " 'rank_test_score': array([7, 2, 1, 2, 4, 6, 5]),\n",
       " 'split0_test_score': array([0.58536585, 0.58536585, 0.59756098, 0.57317073, 0.57317073,\n",
       "        0.54878049, 0.53658537]),\n",
       " 'split0_train_score': array([1.        , 0.67880795, 0.66556291, 0.62582781, 0.61589404,\n",
       "        0.61258278, 0.58609272]),\n",
       " 'split1_test_score': array([0.525 , 0.575 , 0.6   , 0.5875, 0.575 , 0.5625, 0.5625]),\n",
       " 'split1_train_score': array([1.        , 0.70065789, 0.6875    , 0.63157895, 0.625     ,\n",
       "        0.59539474, 0.56578947]),\n",
       " 'split2_test_score': array([0.51948052, 0.57142857, 0.58441558, 0.55844156, 0.55844156,\n",
       "        0.53246753, 0.55844156]),\n",
       " 'split2_train_score': array([1.        , 0.71661238, 0.68078176, 0.65472313, 0.63517915,\n",
       "        0.60586319, 0.57980456]),\n",
       " 'split3_test_score': array([0.57534247, 0.61643836, 0.60273973, 0.57534247, 0.57534247,\n",
       "        0.5890411 , 0.60273973]),\n",
       " 'split3_train_score': array([1.        , 0.70418006, 0.67524116, 0.63987138, 0.62700965,\n",
       "        0.60450161, 0.56913183]),\n",
       " 'split4_test_score': array([0.56944444, 0.59722222, 0.625     , 0.65277778, 0.63888889,\n",
       "        0.61111111, 0.61111111]),\n",
       " 'split4_train_score': array([1.        , 0.68589744, 0.66987179, 0.63782051, 0.62820513,\n",
       "        0.59294872, 0.57371795]),\n",
       " 'std_fit_time': array([0.00195039, 0.00206024, 0.00217889, 0.00189756, 0.00211457,\n",
       "        0.00064421, 0.00110493]),\n",
       " 'std_score_time': array([0.00299918, 0.00237446, 0.00222119, 0.00441199, 0.00158615,\n",
       "        0.00366009, 0.00364101]),\n",
       " 'std_test_score': array([0.02747869, 0.01616025, 0.01288971, 0.03223586, 0.02741546,\n",
       "        0.0277653 , 0.02809087]),\n",
       " 'std_train_score': array([0.        , 0.01344269, 0.00777125, 0.00972196, 0.00621188,\n",
       "        0.00718887, 0.00730756])}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 3}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best Parameter : No. of neighbours=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6015625"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=3, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf=KNeighborsClassifier(n_neighbors=3)\n",
    "knn_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.6770833333333334\n",
      "Test Recall score: 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = knn_clf.predict(X_train_pca)\n",
    "y_pred_test = knn_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN shows an improved model accuracy and fit after applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(lreg_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.11031833, 0.10719943, 0.07631512, 0.03818297, 0.02939463,\n",
       "        0.03852129, 0.07358351, 0.0974535 ]),\n",
       " 'mean_score_time': array([0.00069385, 0.00091662, 0.00110354, 0.00130067, 0.00128832,\n",
       "        0.00148611, 0.00131269, 0.00101109]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.609375  ,\n",
       "        0.70572917, 0.6875    , 0.64583333]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.64196531,\n",
       "        0.82880072, 0.95507739, 0.9973978 ]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 4, 1, 2, 3]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.59756098,\n",
       "        0.68292683, 0.68292683, 0.63414634]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.63907285,\n",
       "        0.82781457, 0.95364238, 0.99668874]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.6   , 0.7   , 0.6375, 0.6125]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.65131579,\n",
       "        0.83881579, 0.95065789, 0.99671053]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.58441558,\n",
       "        0.67532468, 0.66233766, 0.62337662]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.64495114,\n",
       "        0.82410423, 0.96416938, 1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.61643836,\n",
       "        0.7260274 , 0.73972603, 0.69863014]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.63987138,\n",
       "        0.81993569, 0.95819936, 1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.65277778,\n",
       "        0.75      , 0.72222222, 0.66666667]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.63461538,\n",
       "        0.83333333, 0.94871795, 0.99358974]),\n",
       " 'std_fit_time': array([0.01562986, 0.01214148, 0.01074494, 0.00326708, 0.00141227,\n",
       "        0.00114865, 0.00658925, 0.01322707]),\n",
       " 'std_score_time': array([0.00059189, 0.00055978, 0.00023865, 0.00025864, 0.00025085,\n",
       "        0.00051174, 0.00025482, 0.00032867]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02314586,\n",
       "        0.02736869, 0.03737424, 0.03119403]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00571172,\n",
       "        0.0066703 , 0.00555778, 0.00240912])}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7057291666666666"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='multinomial',\n",
       "          n_jobs=1, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_clf = LogisticRegression(multi_class='multinomial', solver='lbfgs',C=1)\n",
    "lreg_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.8203125\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = lreg_clf.predict(X_train_pca)\n",
    "y_pred_test = lreg_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistics Regression shows an improved model accuracy and fit after applying PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "LSVC_clf = LinearSVC(multi_class='crammer_singer')\n",
    "\n",
    "param_grid = {'C': [0.00001,0.0001,0.001,0.01,0.1,1,10,100]}\n",
    "\n",
    "grid_search = GridSearchCV(LSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.0082274 , 0.01229348, 0.02872224, 0.03746872, 0.04009824,\n",
       "        0.11683884, 0.38283691, 0.46555071]),\n",
       " 'mean_score_time': array([0.00225077, 0.00186863, 0.00100436, 0.00149975, 0.00176778,\n",
       "        0.0011878 , 0.00130968, 0.00140285]),\n",
       " 'mean_test_score': array([0.6171875 , 0.61458333, 0.61458333, 0.6171875 , 0.69791667,\n",
       "        0.70052083, 0.63020833, 0.59114583]),\n",
       " 'mean_train_score': array([0.64643744, 0.64514494, 0.64448269, 0.65356614, 0.77480555,\n",
       "        0.91137569, 0.98760332, 1.        ]),\n",
       " 'param_C': masked_array(data=[1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n",
       "              mask=[False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 1e-05},\n",
       "  {'C': 0.0001},\n",
       "  {'C': 0.001},\n",
       "  {'C': 0.01},\n",
       "  {'C': 0.1},\n",
       "  {'C': 1},\n",
       "  {'C': 10},\n",
       "  {'C': 100}],\n",
       " 'rank_test_score': array([4, 6, 6, 4, 2, 1, 3, 8]),\n",
       " 'split0_test_score': array([0.63414634, 0.62195122, 0.62195122, 0.62195122, 0.68292683,\n",
       "        0.65853659, 0.63414634, 0.6097561 ]),\n",
       " 'split0_train_score': array([0.63907285, 0.63907285, 0.63576159, 0.63907285, 0.78476821,\n",
       "        0.89735099, 0.98675497, 1.        ]),\n",
       " 'split1_test_score': array([0.575 , 0.575 , 0.575 , 0.5875, 0.675 , 0.7375, 0.6   , 0.55  ]),\n",
       " 'split1_train_score': array([0.64473684, 0.64473684, 0.64473684, 0.65131579, 0.77960526,\n",
       "        0.91118421, 0.98355263, 1.        ]),\n",
       " 'split2_test_score': array([0.61038961, 0.61038961, 0.61038961, 0.61038961, 0.7012987 ,\n",
       "        0.67532468, 0.63636364, 0.61038961]),\n",
       " 'split2_train_score': array([0.64820847, 0.64495114, 0.64495114, 0.65798046, 0.76872964,\n",
       "        0.91205212, 0.98697068, 1.        ]),\n",
       " 'split3_test_score': array([0.60273973, 0.60273973, 0.60273973, 0.60273973, 0.69863014,\n",
       "        0.69863014, 0.64383562, 0.63013699]),\n",
       " 'split3_train_score': array([0.65273312, 0.65273312, 0.65273312, 0.67202572, 0.76848875,\n",
       "        0.92282958, 0.9903537 , 1.        ]),\n",
       " 'split4_test_score': array([0.66666667, 0.66666667, 0.66666667, 0.66666667, 0.73611111,\n",
       "        0.73611111, 0.63888889, 0.55555556]),\n",
       " 'split4_train_score': array([0.6474359 , 0.64423077, 0.64423077, 0.6474359 , 0.7724359 ,\n",
       "        0.91346154, 0.99038462, 1.        ]),\n",
       " 'std_fit_time': array([0.00167174, 0.00205224, 0.00189641, 0.0038783 , 0.0042161 ,\n",
       "        0.0186847 , 0.07299526, 0.07904059]),\n",
       " 'std_score_time': array([0.00111663, 0.00141557, 0.00084011, 0.00136655, 0.0012461 ,\n",
       "        0.00022771, 0.00024645, 0.00039117]),\n",
       " 'std_test_score': array([0.03066247, 0.02961142, 0.02961142, 0.02639022, 0.02081613,\n",
       "        0.03204328, 0.01582572, 0.03205367]),\n",
       " 'std_train_score': array([0.00449218, 0.00437014, 0.0053754 , 0.01107196, 0.00639884,\n",
       "        0.00815756, 0.00256235, 0.        ])}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : C=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7005208333333334"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='crammer_singer', penalty='l2', random_state=None,\n",
       "     tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSVC_clf = LinearSVC(multi_class='crammer_singer', C=1)\n",
    "LSVC_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.90625\n",
      "Test Recall score: 0.6911764705882353\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = LSVC_clf.predict(X_train_pca)\n",
    "y_pred_test = LSVC_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM still overfits the model after applying PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalised SVM with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'C': [0.0001, 0.001, 0.01, 0.1, 1, 10], 'gamma': [0.0001, 0.001, 0.01, 0.1, 1, 10]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn import svm\n",
    "\n",
    "KSVC_clf = svm.SVC(kernel='rbf')\n",
    "\n",
    "param_grid = {'C': [0.0001,0.001,0.01,0.1,1,10],\n",
    "          'gamma': [0.0001,0.001,0.01,0.1,1,10]}\n",
    "\n",
    "grid_search = GridSearchCV(KSVC_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.02329803, 0.0260983 , 0.0290565 , 0.02382956, 0.02944183,\n",
       "        0.02440333, 0.0264667 , 0.02654343, 0.02495589, 0.0258193 ,\n",
       "        0.02739878, 0.03085971, 0.02205687, 0.0229527 , 0.0263104 ,\n",
       "        0.0353972 , 0.07219791, 0.08513908, 0.02503438, 0.02531457,\n",
       "        0.02713728, 0.04462214, 0.08973494, 0.12169266, 0.02132483,\n",
       "        0.03607206, 0.03710036, 0.04484215, 0.11353149, 0.12918143,\n",
       "        0.03735271, 0.03912077, 0.03170319, 0.04793549, 0.11100459,\n",
       "        0.12312293]),\n",
       " 'mean_score_time': array([0.00608106, 0.00779424, 0.00708632, 0.00582104, 0.00602326,\n",
       "        0.00573044, 0.00624123, 0.00632792, 0.00571537, 0.0059813 ,\n",
       "        0.00621748, 0.00603247, 0.00539956, 0.00482039, 0.00549788,\n",
       "        0.00641723, 0.00794048, 0.00711451, 0.00592742, 0.00571718,\n",
       "        0.00548635, 0.00619373, 0.00919356, 0.00772805, 0.00502434,\n",
       "        0.00610871, 0.00691032, 0.00663772, 0.00780749, 0.00787063,\n",
       "        0.00731897, 0.00671263, 0.00600281, 0.00732794, 0.0067771 ,\n",
       "        0.00763092]),\n",
       " 'mean_test_score': array([0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.53385417, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.65104167, 0.53385417, 0.53385417,\n",
       "        0.53385417, 0.53385417, 0.69791667, 0.70833333, 0.5390625 ,\n",
       "        0.53385417]),\n",
       " 'mean_train_score': array([0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.53393884, 0.53393884, 0.53393884, 0.53393884,\n",
       "        0.53393884, 0.5371777 , 0.73636858, 0.94210098, 1.        ,\n",
       "        0.53393884, 0.542369  , 0.75719163, 0.96878148, 1.        ,\n",
       "        1.        ]),\n",
       " 'param_C': masked_array(data=[0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.0001, 0.001,\n",
       "                    0.001, 0.001, 0.001, 0.001, 0.001, 0.01, 0.01, 0.01,\n",
       "                    0.01, 0.01, 0.01, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 1, 1,\n",
       "                    1, 1, 1, 1, 10, 10, 10, 10, 10, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01,\n",
       "                    0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1, 10, 0.0001,\n",
       "                    0.001, 0.01, 0.1, 1, 10, 0.0001, 0.001, 0.01, 0.1, 1,\n",
       "                    10, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'C': 0.0001, 'gamma': 0.0001},\n",
       "  {'C': 0.0001, 'gamma': 0.001},\n",
       "  {'C': 0.0001, 'gamma': 0.01},\n",
       "  {'C': 0.0001, 'gamma': 0.1},\n",
       "  {'C': 0.0001, 'gamma': 1},\n",
       "  {'C': 0.0001, 'gamma': 10},\n",
       "  {'C': 0.001, 'gamma': 0.0001},\n",
       "  {'C': 0.001, 'gamma': 0.001},\n",
       "  {'C': 0.001, 'gamma': 0.01},\n",
       "  {'C': 0.001, 'gamma': 0.1},\n",
       "  {'C': 0.001, 'gamma': 1},\n",
       "  {'C': 0.001, 'gamma': 10},\n",
       "  {'C': 0.01, 'gamma': 0.0001},\n",
       "  {'C': 0.01, 'gamma': 0.001},\n",
       "  {'C': 0.01, 'gamma': 0.01},\n",
       "  {'C': 0.01, 'gamma': 0.1},\n",
       "  {'C': 0.01, 'gamma': 1},\n",
       "  {'C': 0.01, 'gamma': 10},\n",
       "  {'C': 0.1, 'gamma': 0.0001},\n",
       "  {'C': 0.1, 'gamma': 0.001},\n",
       "  {'C': 0.1, 'gamma': 0.01},\n",
       "  {'C': 0.1, 'gamma': 0.1},\n",
       "  {'C': 0.1, 'gamma': 1},\n",
       "  {'C': 0.1, 'gamma': 10},\n",
       "  {'C': 1, 'gamma': 0.0001},\n",
       "  {'C': 1, 'gamma': 0.001},\n",
       "  {'C': 1, 'gamma': 0.01},\n",
       "  {'C': 1, 'gamma': 0.1},\n",
       "  {'C': 1, 'gamma': 1},\n",
       "  {'C': 1, 'gamma': 10},\n",
       "  {'C': 10, 'gamma': 0.0001},\n",
       "  {'C': 10, 'gamma': 0.001},\n",
       "  {'C': 10, 'gamma': 0.01},\n",
       "  {'C': 10, 'gamma': 0.1},\n",
       "  {'C': 10, 'gamma': 1},\n",
       "  {'C': 10, 'gamma': 10}],\n",
       " 'rank_test_score': array([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
       "        5, 5, 5, 5, 5, 3, 5, 5, 5, 5, 2, 1, 4, 5]),\n",
       " 'split0_test_score': array([0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.5       , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.6097561 , 0.5       , 0.5       ,\n",
       "        0.5       , 0.5       , 0.69512195, 0.69512195, 0.51219512,\n",
       "        0.5       ]),\n",
       " 'split0_train_score': array([0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54304636, 0.54304636, 0.54304636, 0.54304636,\n",
       "        0.54304636, 0.54635762, 0.73509934, 0.94039735, 1.        ,\n",
       "        0.54304636, 0.54966887, 0.7615894 , 0.96688742, 1.        ,\n",
       "        1.        ]),\n",
       " 'split1_test_score': array([0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.5125, 0.5125, 0.5125, 0.6375, 0.5125, 0.5125, 0.5125, 0.5125,\n",
       "        0.6875, 0.725 , 0.5   , 0.5125]),\n",
       " 'split1_train_score': array([0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.53947368, 0.53947368, 0.53947368,\n",
       "        0.53947368, 0.53947368, 0.74342105, 0.94736842, 1.        ,\n",
       "        0.53947368, 0.54276316, 0.75657895, 0.97368421, 1.        ,\n",
       "        1.        ]),\n",
       " 'split2_test_score': array([0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.53246753, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.62337662, 0.53246753, 0.53246753,\n",
       "        0.53246753, 0.53246753, 0.66233766, 0.7012987 , 0.54545455,\n",
       "        0.53246753]),\n",
       " 'split2_train_score': array([0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53420195, 0.53420195, 0.53420195, 0.53420195,\n",
       "        0.53420195, 0.53745928, 0.73941368, 0.95114007, 1.        ,\n",
       "        0.53420195, 0.54397394, 0.75895765, 0.97394137, 1.        ,\n",
       "        1.        ]),\n",
       " 'split3_test_score': array([0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.56164384, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.65753425, 0.56164384, 0.56164384,\n",
       "        0.56164384, 0.56164384, 0.69863014, 0.68493151, 0.57534247,\n",
       "        0.56164384]),\n",
       " 'split3_train_score': array([0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.52733119, 0.52733119, 0.52733119, 0.52733119,\n",
       "        0.52733119, 0.53054662, 0.73954984, 0.93890675, 1.        ,\n",
       "        0.52733119, 0.53697749, 0.75562701, 0.97106109, 1.        ,\n",
       "        1.        ]),\n",
       " 'split4_test_score': array([0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.56944444, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.73611111, 0.56944444, 0.56944444,\n",
       "        0.56944444, 0.56944444, 0.75      , 0.73611111, 0.56944444,\n",
       "        0.56944444]),\n",
       " 'split4_train_score': array([0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.52564103, 0.52564103, 0.52564103, 0.52564103,\n",
       "        0.52564103, 0.53205128, 0.72435897, 0.93269231, 1.        ,\n",
       "        0.52564103, 0.53846154, 0.75320513, 0.95833333, 1.        ,\n",
       "        1.        ]),\n",
       " 'std_fit_time': array([0.0018613 , 0.00425332, 0.00181776, 0.00441197, 0.0027462 ,\n",
       "        0.00328747, 0.00270113, 0.00196405, 0.00221168, 0.00402358,\n",
       "        0.00364984, 0.00179832, 0.00454236, 0.00523145, 0.0040628 ,\n",
       "        0.00212324, 0.0049887 , 0.00938388, 0.00293096, 0.00407807,\n",
       "        0.00272847, 0.0017548 , 0.01324731, 0.01438884, 0.00154913,\n",
       "        0.00509268, 0.00532616, 0.00640089, 0.00907215, 0.01657288,\n",
       "        0.00166711, 0.00389439, 0.0021098 , 0.0044732 , 0.02053444,\n",
       "        0.00719379]),\n",
       " 'std_score_time': array([0.00203768, 0.00224995, 0.00176124, 0.00247713, 0.00049933,\n",
       "        0.00070125, 0.000477  , 0.00068448, 0.00050376, 0.00057614,\n",
       "        0.00092929, 0.00084965, 0.0007359 , 0.00073068, 0.00112264,\n",
       "        0.00073136, 0.00097782, 0.00173695, 0.00080412, 0.00144112,\n",
       "        0.00122738, 0.00198345, 0.00306947, 0.00165313, 0.00031284,\n",
       "        0.00144584, 0.00142927, 0.00058259, 0.00111215, 0.00227581,\n",
       "        0.0004002 , 0.00113672, 0.0008689 , 0.00115394, 0.0018619 ,\n",
       "        0.00058315]),\n",
       " 'std_test_score': array([0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02691538, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.04382431, 0.02691538, 0.02691538,\n",
       "        0.02691538, 0.02691538, 0.02805047, 0.01881092, 0.03005872,\n",
       "        0.02691538]),\n",
       " 'std_train_score': array([0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00672555, 0.00672555, 0.00672555, 0.00672555,\n",
       "        0.00672555, 0.00565461, 0.00655701, 0.00649738, 0.        ,\n",
       "        0.00672555, 0.00448009, 0.00286966, 0.00580705, 0.        ,\n",
       "        0.        ])}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 10, 'gamma': 0.1}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : C=0.0001, gamma=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7083333333333334"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma=0.1, kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "KSVC_clf = svm.SVC(kernel='rbf',C=10,gamma=0.1)\n",
    "KSVC_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.9661458333333334\n",
      "Test Recall score: 0.6764705882352942\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = KSVC_clf.predict(X_train_pca)\n",
    "y_pred_test = KSVC_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernalized SVM still overfits the model after applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Trees with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=None, shuffle=False),\n",
       "       error_score='raise',\n",
       "       estimator=DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [2, 3, 4, 5, 6, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring='recall_weighted', verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "param_grid = {'max_depth': [2, 3, 4, 5,6, 10, 20]}\n",
    "\n",
    "grid_search = GridSearchCV(dt_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.01455064, 0.02011843, 0.02934551, 0.03300323, 0.03734202,\n",
       "        0.06519251, 0.0579257 ]),\n",
       " 'mean_score_time': array([0.00124216, 0.00110126, 0.00103359, 0.00140519, 0.00120311,\n",
       "        0.00236754, 0.00131073]),\n",
       " 'mean_test_score': array([0.53385417, 0.52864583, 0.53385417, 0.5390625 , 0.50260417,\n",
       "        0.43489583, 0.43229167]),\n",
       " 'mean_train_score': array([0.58802314, 0.63098482, 0.70465248, 0.77037599, 0.82632373,\n",
       "        0.94215768, 1.        ]),\n",
       " 'param_max_depth': masked_array(data=[2, 3, 4, 5, 6, 10, 20],\n",
       "              mask=[False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2},\n",
       "  {'max_depth': 3},\n",
       "  {'max_depth': 4},\n",
       "  {'max_depth': 5},\n",
       "  {'max_depth': 6},\n",
       "  {'max_depth': 10},\n",
       "  {'max_depth': 20}],\n",
       " 'rank_test_score': array([2, 4, 2, 1, 5, 6, 7]),\n",
       " 'split0_test_score': array([0.45121951, 0.43902439, 0.45121951, 0.45121951, 0.40243902,\n",
       "        0.26829268, 0.34146341]),\n",
       " 'split0_train_score': array([0.60264901, 0.64569536, 0.73178808, 0.79139073, 0.83774834,\n",
       "        0.96357616, 1.        ]),\n",
       " 'split1_test_score': array([0.55  , 0.5625, 0.5625, 0.6   , 0.5   , 0.45  , 0.45  ]),\n",
       " 'split1_train_score': array([0.59539474, 0.64473684, 0.72368421, 0.78947368, 0.84539474,\n",
       "        0.94407895, 1.        ]),\n",
       " 'split2_test_score': array([0.54545455, 0.54545455, 0.53246753, 0.54545455, 0.53246753,\n",
       "        0.49350649, 0.4025974 ]),\n",
       " 'split2_train_score': array([0.58631922, 0.61889251, 0.69055375, 0.75895765, 0.82410423,\n",
       "        0.93159609, 1.        ]),\n",
       " 'split3_test_score': array([0.5890411 , 0.54794521, 0.53424658, 0.54794521, 0.54794521,\n",
       "        0.50684932, 0.47945205]),\n",
       " 'split3_train_score': array([0.59485531, 0.62700965, 0.6977492 , 0.76205788, 0.80385852,\n",
       "        0.91961415, 1.        ]),\n",
       " 'split4_test_score': array([0.54166667, 0.55555556, 0.59722222, 0.55555556, 0.54166667,\n",
       "        0.47222222, 0.5       ]),\n",
       " 'split4_train_score': array([0.56089744, 0.61858974, 0.67948718, 0.75      , 0.82051282,\n",
       "        0.95192308, 1.        ]),\n",
       " 'std_fit_time': array([0.00090655, 0.0022815 , 0.0017765 , 0.00325013, 0.00423194,\n",
       "        0.00813883, 0.00950916]),\n",
       " 'std_score_time': array([0.00039668, 0.0002021 , 0.00093299, 0.00049032, 0.00074985,\n",
       "        0.00073004, 0.00074745]),\n",
       " 'std_test_score': array([0.04615068, 0.04708689, 0.04881029, 0.04995586, 0.05476894,\n",
       "        0.08893264, 0.05732538]),\n",
       " 'std_train_score': array([0.01451649, 0.01200985, 0.01989105, 0.01685878, 0.01440558,\n",
       "        0.01534642, 0.        ])}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 5}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameter : max_depth = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5390625"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(max_depth=4)\n",
    "dt_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Recall score: 0.703125\n",
      "Test Recall score: 0.5441176470588235\n"
     ]
    }
   ],
   "source": [
    "y_pred_train = dt_clf.predict(X_train_pca)\n",
    "y_pred_test = dt_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that decision tree performs really bad with PCA. This may be due to information lose due to dimentionality reduction.\n",
    "- Also model is also overfitting as there is difference between train and test score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',n_jobs= -1,random_state=10)\n",
    "\n",
    "param_grid = {'max_features': [25,50,75,100],\n",
    "          'max_depth': [6,8,10,12,14],\n",
    "           'max_leaf_nodes':[20,22,30,50]}\n",
    "\n",
    "grid_search = GridSearchCV(rf_clf, param_grid, scoring = 'recall_weighted',cv=kFold, return_train_score=True)\n",
    "grid_search.fit(X_train_pca,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Best parameters : max_features=50, max_depth=12, max_leaf_nodes=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators=300, criterion='gini',max_features=50,max_depth=12,max_leaf_nodes=50)\n",
    "rf_clf.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = rf_clf.predict(X_train_pca)\n",
    "y_pred_test = rf_clf.predict(X_test_pca)\n",
    "\n",
    "print('Train Recall score: {}'\n",
    "      .format(recall_score(y_train, y_pred_train, average='weighted')))\n",
    "print('Test Recall score: {}'\n",
    "      .format(recall_score(y_test, y_pred_test, average='weighted')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is also overfitting the model after applying PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Results have improved after using PCA. We get the best resulting using Linear SVC after applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effort Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Effort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mehul Patel</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shaurya Trehan</td>\n",
       "      <td>50%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Name Effort\n",
       "0     Mehul Patel    50%\n",
       "1  Shaurya Trehan    50%"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "effort = { 'Name':[\"Mehul Patel\",\"Shaurya Trehan\"],'Effort':[\"50%\",\"50%\"]}\n",
    "effort_table = pd.DataFrame(effort)\n",
    "effort_table = effort_table[['Name', 'Effort']]\n",
    "effort_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
